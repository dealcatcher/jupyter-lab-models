{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and filteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import  pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./ElectronicsProductsPricingData.csv\", encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "df[\"prices.dateSeen\"] = pd.to_datetime(df[\"prices.dateSeen\"], errors='coerce')\n",
    "\n",
    "\n",
    "# compute a percent‑discount column from the price fields\n",
    "# discount_percent = ((regular_price - sale_price) / regular_price) * 100\n",
    "# assume amountMax is the non‑sale price and amountMin the current price;\n",
    "# guard against division by zero.\n",
    "\n",
    "df['discount_percent'] = np.where(\n",
    "    df['prices.amountMax'] > 0,\n",
    "    100 * (df['prices.amountMax'] - df['prices.amountMin']) / df['prices.amountMax'],\n",
    "    0\n",
    ")\n",
    "\n",
    "\n",
    "train = df[df[\"prices.dateSeen\"].dt.year == 2017]\n",
    "test = df[df[\"prices.dateSeen\"].dt.year == 2018]\n",
    "\n",
    "\n",
    "df_2018 = df[df[\"prices.dateSeen\"].dt.year == 2018]\n",
    "df = df[df[\"prices.dateSeen\"].dt.year == 2017]\n",
    "\n",
    "mlflow.end_run()\n",
    "mlflow.set_tracking_uri('http://localhost:5050')\n",
    "\n",
    "\n",
    "df[[\"discount_percent\",\"prices.amountMax\",\"prices.dateSeen\",\"prices.amountMin\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Engineer time features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'] = df['prices.dateSeen'].dt.dayofweek\n",
    "df['month'] = df['prices.dateSeen'].dt.month\n",
    "df['week_of_year'] = df['prices.dateSeen'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "df_2018['day_of_week'] = df_2018['prices.dateSeen'].dt.dayofweek\n",
    "df_2018['month'] = df_2018['prices.dateSeen'].dt.month\n",
    "df_2018['week_of_year'] = df_2018['prices.dateSeen'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "\n",
    "# Binary flag for Thursday updates\n",
    "df['is_thursday'] = (df['day_of_week'] == 3).astype(int)\n",
    "df_2018['is_thursday'] = (df_2018['day_of_week'] == 3).astype(int)\n",
    "\n",
    "# Preview the engineered features\n",
    "df[['prices.dateSeen', 'day_of_week', 'week_of_year', 'is_thursday']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create figure (VERY WIDE)\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=3,\n",
    "    ncols=2,\n",
    "    figsize=(20, 18)   # ← control width here\n",
    ")\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1 Scatter\n",
    "sns.scatterplot(data=df, x=\"prices.dateSeen\", y=\"discount_percent\", ax=axes[0])\n",
    "sns.scatterplot(data=df_2018, x=\"prices.dateSeen\", y=\"discount_percent\", ax=axes[0])\n",
    "\n",
    "\n",
    "# 2 Line\n",
    "sns.lineplot(data=df, x=\"month\", y=\"discount_percent\", ax=axes[1])\n",
    "sns.lineplot(data=df_2018, x=\"month\", y=\"discount_percent\", ax=axes[1])\n",
    "\n",
    "\n",
    "# 3 Bar\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "sns.barplot(data=df, x=\"day_of_week\", y=\"discount_percent\", ax=axes[2], order=range(7))\n",
    "sns.barplot(data=df_2018, x=\"day_of_week\", y=\"discount_percent\", ax=axes[2], order=range(7), alpha=0.5)\n",
    "axes[2].set_xticklabels(weekdays)\n",
    "\n",
    "# 4 Box\n",
    "\n",
    "sns.lineplot(data=df, x=\"week_of_year\", y=\"discount_percent\", ax=axes[3])\n",
    "sns.lineplot(data=df_2018, x=\"week_of_year\", y=\"discount_percent\", ax=axes[3])\n",
    "\n",
    "# 5 Rolling Average (4 weeks)\n",
    "df_rolling_4w = df.set_index('prices.dateSeen').sort_index()\n",
    "df_rolling_4w = df_rolling_4w[\"discount_percent\"].rolling('4D').mean()\n",
    "sns.lineplot(x=df_rolling_4w.index, y=df_rolling_4w , ax=axes[4])\n",
    "sns.lineplot(x=df_2018.set_index('prices.dateSeen').sort_index().index, y=df_2018.set_index('prices.dateSeen').sort_index()[\"discount_percent\"].rolling('4D').mean() , ax=axes[4])\n",
    "\n",
    "\n",
    "\n",
    "df_2018.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# is_thursday\n",
    "sns.barplot(data=df, x=\"is_thursday\", y=\"discount_percent\", ax=axes[5])\n",
    "sns.barplot(data=df_2018, x=\"is_thursday\", y=\"discount_percent\", ax=axes[5], alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Improve spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Temporal Feature Extraction\n",
    "As per the Phase 2 requirements, we focus on high-resolution time features: day_of_week, month, week_of_year, and is_thursday. Justification: Thursdays are often identified as key re-pricing days in retail as stores prepare for weekend traffic. Utility: These features provide the categorical \"hooks\" for the Stage 1 Classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create lag features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adf test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"prices.dateSeen\")\n",
    "df = df.set_index(\"prices.dateSeen\")\n",
    "\n",
    "df_2018 = df_2018.sort_values(\"prices.dateSeen\")\n",
    "df_2018 = df_2018.set_index(\"prices.dateSeen\")\n",
    "\n",
    "\n",
    "## Here the data is resampled to daily frequency, and the mean discount percentage for each day is calculated.\n",
    "## This is important because the original data may have multiple entries per day, and we want to analyze the overall trend of discounts on a daily basis.\n",
    "daily_series = df[\"discount_percent\"].resample(\"D\").mean()\n",
    "daily_series = daily_series.dropna()\n",
    "\n",
    "print(\"Length:\", len(daily_series))\n",
    "\n",
    "\n",
    "result = adfuller(daily_series)\n",
    "\n",
    "print(\"ADF Statistic:\", result[0])\n",
    "print(\"p-value:\", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "figure1 = plt.figure(figsize=(12, 6))\n",
    "plot_acf(df_2018[\"discount_percent\"].dropna(), lags=60)\n",
    "plt.title(\"Autocorrelation Function (ACF)\")\n",
    "plt.savefig(\"./images/acf.png\")\n",
    "mlflow.log_artifact(\"./images/acf.png\")\n",
    "plt.close(figure1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partial autocoorelation \n",
    " -- here the lag 1 is the one with the highest correlation , whereas other lag 3 is alo , great too , but after lag 6, they are all below the confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pacf = plt.figure()\n",
    "plot_pacf(df[\"discount_percent\"].dropna(), lags=60)\n",
    "plt.savefig(\"./images/pacf.png\")\n",
    "mlflow.log_artifact(\"./images/pacf.png\")\n",
    "plt.close(fig_pacf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lag 1 and lag 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df[\"discount_percent\"]\n",
    "\n",
    "lag_df = pd.DataFrame({\n",
    "    \"current\": series,\n",
    "    \"lag_1\": series.shift(1),\n",
    "    \"lag_2\": series.shift(2),\n",
    "    \"lag_14\": series.shift(14),\n",
    "    \"lag_30\": series.shift(30),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = lag_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_lag_1 = plt.figure()\n",
    "\n",
    "plt.scatter(lag_df[\"lag_1\"], lag_df[\"current\"], alpha=0.5)\n",
    "plt.xlabel(\"Lag 1 (t-1)\")\n",
    "plt.ylabel(\"Current (t)\")\n",
    "plt.title(\"Lag-1 Relationship\")\n",
    "plt.savefig(\"./images/lag_1.png\")\n",
    "mlflow.log_artifact(\"./images/lag_1.png\")\n",
    "plt.close(figure_lag_1)\n",
    "\n",
    "\n",
    "figure_lag_2 = plt.figure()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(lag_df[\"lag_2\"], lag_df[\"current\"], alpha=0.5)\n",
    "plt.xlabel(\"Lag 2 (t-2)\")\n",
    "plt.ylabel(\"Current (t)\")\n",
    "plt.title(\"Lag-2 Relationship\")\n",
    "plt.savefig(\"./images/lag_2.png\")\n",
    "mlflow.log_artifact(\"./images/lag_2.png\")\n",
    "plt.close(figure_lag_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### montly and peroidic lag analysies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # ── Monthly average discount ──────────────────────────────────────────────\n",
    "monthly_series = df[\"discount_percent\"].resample(\"ME\").mean()\n",
    "monthly_series_2018 = df_2018[\"discount_percent\"].resample(\"ME\").mean()\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "# Plot monthly mean\n",
    "axes[0].plot(monthly_series.index, monthly_series.values, marker='o', linewidth=2, color='steelblue')\n",
    "axes[0].plot(monthly_series_2018.index, monthly_series_2018.values, marker='o', linewidth=2, color='orange')\n",
    "axes[0].set_title(\"Monthly Average Discount %\")\n",
    "axes[0].set_xlabel(\"Month\")\n",
    "axes[0].set_ylabel(\"Avg Discount %\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "# Month-of-year box plot (seasonality)\n",
    "monthly_df = df[\"discount_percent\"].copy().reset_index()\n",
    "monthly_df.columns = [\"date\", \"discount_percent\"]\n",
    "monthly_df[\"month\"] = monthly_df[\"date\"].dt.month\n",
    "monthly_df[\"month_name\"] = monthly_df[\"date\"].dt.strftime(\"%b\")\n",
    "month_order = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
    "            \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "monthly_df[\"month_name\"] = pd.Categorical(monthly_df[\"month_name\"], categories=month_order, ordered=True)\n",
    "monthly_df_clean = monthly_df.dropna(subset=[\"month_name\"])\n",
    "monthly_df_clean.boxplot(column=\"discount_percent\", by=\"month_name\",\n",
    "                        ax=axes[1], grid=False)\n",
    "axes[1].set_title(\"Discount % Distribution by Month\")\n",
    "axes[1].set_xlabel(\"Month\")\n",
    "axes[1].set_ylabel(\"Discount %\")\n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # ── Lag-14 and Lag-30 scatter plots (weekly / monthly lags) ───────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].scatter(lag_df[\"lag_14\"], lag_df[\"current\"], alpha=0.4, color='steelblue')\n",
    "axes[0].set_xlabel(\"Lag 14 (t-14)\")\n",
    "axes[0].set_ylabel(\"Current (t)\")\n",
    "axes[0].set_title(\"Lag-14 Relationship (~2 weeks)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].scatter(lag_df[\"lag_30\"], lag_df[\"current\"], alpha=0.4, color='darkorange')\n",
    "axes[1].set_xlabel(\"Lag 30 (t-30)\")\n",
    "axes[1].set_ylabel(\"Current (t)\")\n",
    "axes[1].set_title(\"Lag-30 Relationship (~1 month)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Lag correlation summary table ─────────────────────────────────────────\n",
    "# Use lag_df which already has duplicates removed via dropna/shift alignment\n",
    "\n",
    "lag_df[\"lag_7\"] = lag_df[\"current\"].shift(7)\n",
    "lag_df_clean = lag_df.dropna()\n",
    "\n",
    "lag_corr = pd.DataFrame({\n",
    "    \"lag\": [1, 2, 7, 14, 30],\n",
    "    \"pearson_r\": [\n",
    "        lag_df_clean[\"lag_1\"].corr(lag_df_clean[\"current\"]),\n",
    "        lag_df_clean[\"lag_2\"].corr(lag_df_clean[\"current\"]),\n",
    "        lag_df_clean[\"lag_7\"].corr(lag_df_clean[\"current\"]),\n",
    "        lag_df_clean[\"lag_14\"].corr(lag_df_clean[\"current\"]),\n",
    "        lag_df_clean[\"lag_30\"].corr(lag_df_clean[\"current\"]),\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "colors = ['steelblue' if r > 0.1 else 'lightgray' for r in lag_corr[\"pearson_r\"]]\n",
    "bars = ax.bar([f\"Lag {l}\" for l in lag_corr[\"lag\"]], lag_corr[\"pearson_r\"], color=colors)\n",
    "ax.axhline(0.1, color='red', linestyle='--', linewidth=1, label='0.1 threshold')\n",
    "ax.set_title(\"Pearson Correlation: Lag vs Current Discount %\")\n",
    "ax.set_ylabel(\"Pearson r\")\n",
    "ax.set_xlabel(\"Lag\")\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, lag_corr[\"pearson_r\"]):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "            f\"{val:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "plt.savefig(\"./images/Pearson_Correlation(Lag_vs_Current_Discount).png\")\n",
    "mlflow.log_artifact(\"./images/Pearson_Correlation(Lag_vs_Current_Discount).png\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(lag_corr.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Handle =% discount - build two stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a percent‑discount column from the price fields\n",
    "# discount_percent = ((regular_price - sale_price) / regular_price) * 100\n",
    "# assume amountMax is the non‑sale price and amountMin the current price;\n",
    "# guard against division by zero.\n",
    "df['discount_percent'] = np.where(\n",
    "\tdf['prices.amountMax'] > 0,\n",
    "\t100 * (df['prices.amountMax'] - df['prices.amountMin']) / df['prices.amountMax'],\n",
    "\t0\n",
    ")\n",
    "\n",
    "# 1. Creating the Binary Classification Target (Stage 1)\n",
    "# 1 = Discounted (Sale), 0 = Not Discounted (Regular Price)\n",
    "df['is_discounted'] = (df['discount_percent'] > 0).astype(int)\n",
    "\n",
    "# 2. Prepare the Regression Target (Stage 2)\n",
    "# We isolate actual discount values to prevent zero-inflation from biasing the model\n",
    "actual_discounts = df.loc[df['is_discounted'] == 1, 'discount_percent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Handling 0% Discounts: Two-Stage Methodology\n",
    "To address the high frequency of zero-discount days, we implement a two-stage approach: 1. Classification (Stage 1): Predicting the probability of a sale occurring based on time features. 2. Regression (Stage 2): Estimating the depth of the discount once a sale event is confirmed. Benefit: This architectural decision minimizes the error (MAE/RMSE) caused by zero-inflated target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='is_discounted', data=df, palette='Set2')\n",
    "plt.title('Distribution of Discount Events (Stage 1 Target)')\n",
    "plt.xlabel('Is there a Discount? (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is two-stage model ?\n",
    "since there are a lot of zeroes inside the data , so its confused by all the zeroes and tends to predict small values like 2-3  % instead of true zeroes or real discounts. we will split the problem into two \n",
    "\n",
    "- Stage 1 (classifier ) : \"will there be a discount at all?\n",
    "- Stage 2 (regression) : \"if yes, how much will the discount be\" -predict the %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train/test split: 2017 = train, 2018 = test\n",
    "\n",
    " - we will split the dat series into two different forms \n",
    " - **2017** - Train for the data \n",
    " - **2018** - Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset containing ONLY actual discounts (Stage 2 data)\n",
    "df_discounted = df[df['is_discounted'] == 1]\n",
    "\n",
    "# Visualization: Distribution of Non-Zero Discounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_discounted['discount_percent'], bins=20, kde=True, color='darkorange')\n",
    "plt.title('Stage 2: Magnitude of Actual Discounts (Excluding 0%)')\n",
    "plt.xlabel('Discount Percentage (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA model\n",
    "The **ARIMA** model consists of three different values , the p(Autoregressive), d(differencing), q(movig average)\n",
    " - **Autoregressive** - it refers to the relationship between the current value and the past vlaues of the sam variable.\n",
    " - **Integration** - it refers to the gegree of differcing applied to the data to make it stationary. \n",
    " - **Moving Average** - it refers to the relationship between the current value and the past errors of the model. \n",
    "\n",
    "From the above data filtering , we have p value either 1, 2,3 or 4 , now for the q value , for the q value , since we from the above ADF test , we get the p-value less then 0.05 , which makes the series to be stationary , d value is 0 , but if the series was greater then 0.05 , then we have to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train = train.sort_values(\"prices.dateSeen\")\n",
    "train = train.set_index(\"prices.dateSeen\")\n",
    "\n",
    "\n",
    "\n",
    "test = test.sort_values(\"prices.dateSeen\")\n",
    "test = test.set_index(\"prices.dateSeen\")\n",
    "\n",
    "\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "mlflow.set_experiment(\"ARIMA_Discount_Model\")\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_aic = float(\"inf\")\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for p in range(1,4):\n",
    "    for d in range(2):\n",
    "        for q in range(3):\n",
    "\n",
    "            with mlflow.start_run():\n",
    "\n",
    "                # Log parameters\n",
    "                mlflow.log_param(\"p\", p)\n",
    "                mlflow.log_param(\"d\", d)\n",
    "                mlflow.log_param(\"q\", q)\n",
    "\n",
    "                model = ARIMA(train[\"discount_percent\"], order=(p,d,q))\n",
    "                model_fit = model.fit()\n",
    "\n",
    "                forecast = model_fit.forecast(len(test))\n",
    "\n",
    "                rmse = np.sqrt(\n",
    "                    mean_squared_error(\n",
    "                        test[\"discount_percent\"], forecast\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                aic = model_fit.aic\n",
    "\n",
    "                # ✅ ALWAYS log metrics\n",
    "                mlflow.log_metric(\"RMSE\", rmse)\n",
    "                mlflow.log_metric(\"AIC\", aic)\n",
    "\n",
    "                # ✅ Update best model OUTSIDE logging logic\n",
    "                if (rmse < best_rmse) or (rmse == best_rmse and aic < best_aic):\n",
    "                    best_rmse = rmse\n",
    "                    best_aic = aic\n",
    "                    best_model = model_fit\n",
    "                    best_params = (p,d,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"best_arima_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "mlflow.log_artifact(\"best_arima_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"Best_Model\"):\n",
    "\n",
    "    mlflow.log_param(\"best_p\", best_params[0])\n",
    "    mlflow.log_param(\"best_d\", best_params[1])\n",
    "    mlflow.log_param(\"best_q\", best_params[2])\n",
    "\n",
    "    mlflow.log_metric(\"best_RMSE\", best_rmse)\n",
    "    mlflow.log_metric(\"best_AIC\", best_aic)\n",
    "\n",
    "    mlflow.log_artifact(\"best_arima_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
