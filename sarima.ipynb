{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Time Series Analysis: Electronics Product Pricing Data\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive time series analysis on electronics product pricing data with the goal of understanding discount patterns, detecting seasonality, and preparing data for SARIMA forecasting.\n",
    "\n",
    "### Analysis Components:\n",
    "1. **Data Loading and Preprocessing**\n",
    "2. **Feature Engineering** (discount calculations, temporal features)\n",
    "3. **Exploratory Data Analysis** (correlations, visualizations)\n",
    "4. **Seasonality Detection** (monthly and weekly patterns)\n",
    "5. **Rolling Statistics** (weekly aggregations)\n",
    "6. **Lag Feature Engineering** (autocorrelation analysis)\n",
    "\n",
    "### Dataset:\n",
    "- **Source**: ElectronicsProductsPricingData.csv\n",
    "- **Time Period**: 2014-2018 (focus on 2017-2018)\n",
    "- **Key Metric**: Discount percentage over time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Loading\n",
    "\n",
    "This section imports the required libraries and loads the dataset into a pandas DataFrame. The dataset is the foundation for all subsequent preprocessing and analysis steps. We will install all the libraries that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Installation\n",
    "\n",
    "**Purpose**: Install required Python packages for time series analysis.\n",
    "\n",
    "**Libraries needed**:\n",
    "- **`statsmodels`**: Statistical models for time series (ARIMA, SARIMA, ADF test, ACF/PACF)\n",
    "- **`pandas`**: Data manipulation and analysis with DataFrames\n",
    "- **`matplotlib`**: Core plotting library for visualizations\n",
    "- **`seaborn`**: Statistical visualization built on matplotlib\n",
    "\n",
    "**Installation command**:\n",
    "```bash\n",
    "pip install statsmodels pandas matplotlib seaborn\n",
    "```\n",
    "\n",
    "**Note**: This cell is commented out because package installation should typically be done via terminal/command prompt, not within the notebook itself. Uncomment only if running in an environment where pip install in cells is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install statsmodels pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading & Filtering data \n",
    "\n",
    "### 1.1 Load data\n",
    "Reads a CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Display Info\n",
    "\n",
    "**Purpose**: Import necessary libraries, load the electronics pricing dataset, and inspect its structure.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Import Core Libraries**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import matplotlib.pyplot as plt\n",
    "   import seaborn as sns\n",
    "   ```\n",
    "   - `pandas`: For data manipulation and analysis\n",
    "   - `matplotlib.pyplot`: For creating plots and visualizations\n",
    "   - `seaborn`: For statistical visualizations and attractive default styles\n",
    "\n",
    "2. **Load CSV File**:\n",
    "   ```python\n",
    "   df = pd.read_csv(\"./ElectronicsProductsPricingData.csv\", encoding='latin1')\n",
    "   ```\n",
    "   - Reads the CSV file from the current directory\n",
    "   - `encoding='latin1'`: Handles special characters in product names (accents, symbols)\n",
    "   - Stores data in DataFrame `df`\n",
    "\n",
    "3. **Display Dataset Information**:\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "   - Shows comprehensive dataset summary:\n",
    "     - Number of rows and columns\n",
    "     - Column names and their data types\n",
    "     - Non-null counts (identifies missing values)\n",
    "     - Memory usage\n",
    "\n",
    "**Expected Output**:\n",
    "- DataFrame index range\n",
    "- List of all columns with their types (int64, float64, object)\n",
    "- Count of non-null values per column\n",
    "- Total memory usage\n",
    "\n",
    "**Key columns to note**:\n",
    "- Price-related: `prices.amountMin`, `prices.amountMax`, `prices.isSale`\n",
    "- Date-related: `prices.dateSeen`, `dateAdded`, `dateUpdated`\n",
    "- Product info: `id`, `brand`, `categories`, `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./ElectronicsProductsPricingData.csv\", encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Remove Unnecessary Columns\n",
    "\n",
    "**Purpose**: Clean the dataset by dropping columns that are not needed for price and discount analysis.\n",
    "\n",
    "**Columns being removed**:\n",
    "\n",
    "1. **`Unnamed: 26-30`**: Empty columns likely created during data export\n",
    "2. **`sourceURLs`**: Web source links (not needed for analysis)\n",
    "3. **`prices.currency`**: Currency type (assuming all same currency)\n",
    "4. **`keys`**: Internal database keys\n",
    "5. **`ean`**: European Article Number (barcode)\n",
    "6. **`prices.shipping`**: Shipping costs (focusing on product price only)\n",
    "7. **`manufacturerNumber`**: Manufacturer-specific product codes\n",
    "8. **`upc`**: Universal Product Code (another barcode standard)\n",
    "\n",
    "**Why remove these columns**:\n",
    "- **Reduces memory**: Smaller dataset is faster to process\n",
    "- **Improves clarity**: Easier to see relevant columns\n",
    "- **Focuses analysis**: Only keeps features needed for time series modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing all the unnecessary columns from the dataframe\n",
    "df.drop(columns=[\"Unnamed: 26\",\"Unnamed: 27\",\"Unnamed: 28\",\"Unnamed: 29\",\"Unnamed: 30\",\"sourceURLs\",\"prices.currency\", \"keys\",\"ean\",\"prices.shipping\",\"manufacturerNumber\",\"upc\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculate Price Difference and Discount Percentage\n",
    "\n",
    "**Purpose**: Create new features to quantify discounts for each product.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Remove rows with missing prices**:\n",
    "\n",
    "2. **Calculate absolute price difference**:\n",
    "   \n",
    "3. **Calculate discount percentage**:\n",
    "   \n",
    "4. **Display updated info**:\n",
    "   \n",
    "\n",
    "**Why discount percentage matters**:\n",
    "- **Time series target**: This is the primary metric we'll forecast\n",
    "- **Comparability**: 20% discount means the same whether on $10 or $1000 item\n",
    "- **Business relevance**: Reflects actual promotional strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##1.2 we will check if the prices amountMax and minAmount is same \n",
    "## we have also put a new column discount percentage.\n",
    "\n",
    "df.dropna(subset=[\"prices.amountMax\", \"prices.amountMin\"], inplace=True)\n",
    "df[\"price_difference\"] = df[\"prices.amountMax\"] - df[\"prices.amountMin\"]\n",
    "df['discount_percent'] = (df['price_difference'] / df['prices.amountMax']) * 100\n",
    " \n",
    "df.info()\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Analyze Product ID Distribution\n",
    "\n",
    "**Purpose**: Identify which products have multiple price entries (good candidates for time series analysis).\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "- `value_counts()`: Counts occurrences of each unique product ID\n",
    "- Returns a Series sorted by frequency (descending)\n",
    "- Shows which products appear most often in the dataset\n",
    "\n",
    "**What the output tells us**:\n",
    "\n",
    "- **High counts** (e.g., 150+ entries):\n",
    "  - Product with extensive price history\n",
    "  - Multiple observations over time\n",
    "  - **Excellent for time series modeling**\n",
    "  - Can detect trends and seasonality\n",
    "\n",
    "- **Medium counts** (e.g., 20-50 entries):\n",
    "  - Reasonable historical data\n",
    "  - Sufficient for basic trend analysis\n",
    "\n",
    "- **Low counts** (e.g., 1-5 entries):\n",
    "  - Limited history\n",
    "  - Not suitable for time series modeling\n",
    "  - May be new products or single observations\n",
    "\n",
    "**Why this matters**:\n",
    "- **Product selection**: Helps choose which products to analyze in detail\n",
    "- **Data richness**: Products with more entries provide better forecasts\n",
    "- **Time series viability**: Need sufficient data points for SARIMA models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will check if the data has unique ids \n",
    "df[\"id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Filter for Specific Product Case Study\n",
    "\n",
    "**Purpose**: Focus on one product with rich historical data and filter for sale periods.\n",
    "\n",
    "\n",
    "\n",
    "1. **Filter by specific product ID**:\n",
    "2. **Filter for sale items only**:\n",
    "3. **Display results**:\n",
    "  \n",
    "\n",
    "**Why this approach**:\n",
    "\n",
    "- **Case study methodology**: Deep dive into one product before generalizing\n",
    "- **Data richness**: This product has sufficient history for analysis\n",
    "- **Sale focus**: Understand promotional pricing strategy specifically\n",
    "- **Simpler modeling**: Single product is easier than multi-product aggregation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking in the dataframe electronics if there are different products\n",
    "## from the above category, we have checked if the dataframe has different ids, and then checked how many values each id has, where we found one particular interested one. \n",
    "\n",
    "\n",
    "df_id = df[df[\"id\"] == \"AV1YFZVDvKc47QAVgp7V\"]\n",
    "df_id = df_id[df_id[\"prices.isSale\"] == True]\n",
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6  Date Format and Discount Calculation\n",
    "\n",
    "Proper date handling is crucial for time series analysis. This section converts string dates to datetime objects and extracts temporal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Parsing and Temporal Feature Engineering\n",
    "\n",
    "**Purpose**: Convert date strings to datetime objects and extract year components for temporal analysis.\n",
    "\n",
    "1. **Convert three date columns to datetime format**:\n",
    "   \n",
    "   \n",
    "   **Each date field serves a different purpose**:\n",
    "   - **`dateSeen`**: When the price was actually observed/scraped\n",
    "     - Most relevant for understanding consumer-facing prices\n",
    "     - Best for analyzing real-world pricing trends\n",
    "   \n",
    "   - **`dateUpdated`**: When the database record was last modified\n",
    "     - Shows data freshness\n",
    "     - May lag behind actual price changes\n",
    "   \n",
    "   - **`dateAdded`**: When product first entered the database\n",
    "     - Useful for understanding product lifecycle\n",
    "     - Initial pricing analysis\n",
    "   \n",
    "   **`errors=\"coerce\"`**: Invalid dates become NaT (Not a Time) instead of raising errors\n",
    "\n",
    "2. **Extract year from each datetime column**:\n",
    "  \n",
    "   - `.dt.year`: Accessor for datetime year component\n",
    "   - Creates integer columns (2014, 2015, 2016, etc.)\n",
    "   - Enables easy year-based filtering and grouping\n",
    "\n",
    "3. **Check year distribution**:\n",
    " \n",
    "   - Shows how many records per year\n",
    "   - Identifies temporal coverage of dataset\n",
    "   - Helps decide which years to focus on\n",
    "\n",
    "4. **Data validation - remove unparseable dates**:\n",
    "   \n",
    "5. **Remove redundant original column**:\n",
    "  \n",
    "6. **Display cleaned data**:\n",
    "  \n",
    "\n",
    "**Why datetime conversion matters**:\n",
    "- **Enables time-based operations**:\n",
    "  - Sorting chronologically\n",
    "  - Resampling (daily â†’ weekly â†’ monthly)\n",
    "  - Date filtering and slicing\n",
    "  - Time difference calculations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## converting the date columns (which are in strings) to datetime format\n",
    "df[\"dateSeen\"] = pd.to_datetime(df[\"prices.dateSeen\"], errors=\"coerce\")\n",
    "df[\"dateUpdated\"] = pd.to_datetime(df[\"dateUpdated\"], errors=\"coerce\")\n",
    "df[\"dateAdded\"] = pd.to_datetime(df[\"dateAdded\"], errors=\"coerce\")\n",
    "\n",
    "## extracting the year from the date column\n",
    "df[\"date_year\"] = df[\"dateSeen\"].dt.year\n",
    "df[\"updated_year\"] = df[\"dateUpdated\"].dt.year\n",
    "df[\"added_year\"] = df[\"dateAdded\"].dt.year\n",
    "\n",
    "df[\"date_year\"].value_counts()\n",
    "# 2. Drop rows where dates couldn't be parsed (Cleaning the dataset)\n",
    "# This is part of your \"Data validation pipeline\" milestone \n",
    "df = df.dropna(subset=['dateAdded', 'dateUpdated'])\n",
    "\n",
    "## we delete the column prices.dateSeen \n",
    "df.drop(columns=['prices.dateSeen'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATA Visualization \n",
    "\n",
    "Comprehensive visualizations to understand pricing patterns, discount trends, and temporal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploratory Data Analysis: Multi-Dimensional Discount Analysis\n",
    "To identify price volatility and seasonal discounting cycles, we implemented a 2x2 visualization matrix. This aligns with the Week 3-4 Milestone of performing deep exploratory data analysis (EDA) to justify the transition to predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2x2 frame\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('DealCatcher EDA: Price & Discount Trends', fontsize=20)\n",
    "\n",
    "# 1. Scatterplot of Min-Max Prices\n",
    "sns.scatterplot(ax=axes[0, 0], data=df, x='prices.amountMin', y='prices.amountMax', alpha=0.5)\n",
    "axes[0, 0].set_title('Scatterplot of Min-Max Prices')\n",
    "\n",
    "# 2. Discount% vs Date Added\n",
    "sns.scatterplot(ax=axes[0, 1], data=df, x='dateAdded', y='discount_percent', color='orange')\n",
    "axes[0, 1].set_title('Discount % vs Date Added')\n",
    "\n",
    "# 3. Discount% vs Date Last Seen\n",
    "sns.scatterplot(ax=axes[1, 0], data=df, x='dateSeen', y='discount_percent', color='green')\n",
    "axes[1, 0].set_title('Discount % vs Date Seen')\n",
    "\n",
    "# 4. Discount% vs Date Updated\n",
    "sns.scatterplot(ax=axes[1, 1], data=df, x='dateUpdated', y='discount_percent', color='red')\n",
    "axes[1, 1].set_title('Discount % vs Date Updated')\n",
    "\n",
    "# Rotate labels so they don't overlap\n",
    "for ax in axes.flat:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 Correlation Heatmap\n",
    "\n",
    "**Purpose**: Visualize correlations between all numerical variables in the dataset.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Select only numerical columns**:\n",
    "   - Filters DataFrame to keep only numeric data types\n",
    "   - Excludes strings, dates, and boolean columns\n",
    "   - Necessary because correlation requires numerical data\n",
    "\n",
    "2. **Calculate correlation matrix**:\n",
    "  \n",
    "   - Computes Pearson correlation between all numeric pairs\n",
    "   - Values range from -1 to +1\n",
    "\n",
    "3. **Create heatmap visualization**:\n",
    "   ```python\n",
    "   plt.figure(figsize=(6, 4))\n",
    "   sns.heatmap(df_heat.corr(), annot=True, cmap=\"coolwarm\")\n",
    "   ```\n",
    "   - `annot=True`: Shows correlation values in each cell\n",
    "   - `cmap=\"coolwarm\"`: Blue (negative) â†’ White (zero) â†’ Red (positive)\n",
    "\n",
    "**Why this matters for time series**:\n",
    "- Identifies multicollinearity issues\n",
    "- Helps select independent variables\n",
    "- Reveals hidden relationships\n",
    "- Guides feature engineering decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will use the heatmap for the dataset to visualize the correlation between different numerical features\n",
    "## For that we will use seaborn library\n",
    "## Also we will filter out the non-numerical columns from the dataset\n",
    "\n",
    "\n",
    "df_heat = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Electronics Products Pricing Data heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(df_heat.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Electronics Products Pricing Data Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Price Range Visualization (Contains Bug)\n",
    "\n",
    "**Purpose**: Attempt to visualize price ranges over DataFrame indices.\n",
    "\n",
    "**What this code TRIES to do**:\n",
    "- Plot maximum and minimum prices over the dataset index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"dateUpdated\"], df[\"prices.amountMax\"], marker=\"o\", label=\"Amount Max\")\n",
    "plt.plot(df[\"dateUpdated\"], df[\"prices.amountMin\"], marker=\"x\", label=\"Amount Min\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Price Range Over Time\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Seasonal Discount Analysis (2017-2018)\n",
    "\n",
    "**Purpose**: Filter data to 2017-2018 and create two plots showing seasonal discount patterns from different date perspectives.\n",
    "\n",
    "**Part 1: Data Filtering**\n",
    "\n",
    "**Why filter to 2017-2018?**\n",
    "- Earlier analysis showed 2014-2015 had sparse data\n",
    "- 2017-2018 have the highest data density\n",
    "- Ensures all three date fields are within range\n",
    "- Focuses on most recent and complete period\n",
    "\n",
    "**Part 2: Plot 1 - Seasonality by Date Seen**\n",
    "\n",
    "**What this does**:\n",
    "- Extracts month (1-12) from dateSeen\n",
    "- Creates line plot with:\n",
    "  - X-axis: Months (1=Jan, 12=Dec)\n",
    "  - Y-axis: Average discount percentage\n",
    "  - Two lines (one for 2017, one for 2018)\n",
    "  - `marker=\"o\"`: Circle at each month\n",
    "\n",
    "**How to interpret**:\n",
    "- **Peaks**: Months with highest discounts (likely Nov-Dec)\n",
    "- **Valleys**: Months with lowest discounts (likely Jan-Feb)\n",
    "- **Line comparison**: Year-over-year consistency\n",
    "\n",
    "**Part 3: Plot 2 - Seasonality by Date Added**\n",
    "\n",
    "\n",
    "**What this shows**:\n",
    "- Discount patterns based on when products were added\n",
    "- Different perspective from observation dates\n",
    "- May reveal product launch strategies\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**: Two line plots showing monthly discount percentage trends with year-over-year comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove the years 2014 and 2015 from the dataset as they have very few data points and we will focus on the years 2017 and 2018 for our seasonality analysis.\n",
    "\n",
    "\n",
    "## here we will change the data from the electronics dataset to only include the years 2017 and 2018 for the date seen, date updated and date added columns as they have more data points and we will focus on these years for our seasonality analysis.\n",
    "df = df[df[\"updated_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"date_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"added_year\"].isin([2017, 2018])]\n",
    "\n",
    "df[\"month\"] = df[\"dateSeen\"].dt.month\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(data=df, x=\"month\", y=\"discount_percent\", hue=\"date_year\", marker=\"o\")\n",
    "plt.title(\"Discount Seasonality using Date Seen (2017â€“2018)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##\n",
    "df_added = df.copy()\n",
    "df_added[\"month\"] = df_added[\"dateAdded\"].dt.month\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=df_added,\n",
    "    x=\"month\",\n",
    "    y=\"discount_percent\",\n",
    "    hue=\"added_year\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "plt.title(\"Discount Seasonality using Date Added (2017â€“2018)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Discount Percent\")\n",
    "plt.legend(title=\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Create Month Ã— Year Pivot Table\n",
    "\n",
    "**Purpose**: Reorganize data into a pivot table with months as rows and years as columns.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Set dateSeen as index and sort chronologically**:\n",
    "2. **Create pivot table**:\n",
    "\n",
    "\n",
    "**Parameters explained**:\n",
    "- `values=\"discount_percent\"`: The metric to aggregate\n",
    "- `index=...index.month`: Rows = months (1-12)\n",
    "- `columns=...index.year`: Columns = years (2017, 2018)\n",
    "- `aggfunc=\"mean\"`: Calculate average discount per cell\n",
    "\n",
    "\n",
    "\n",
    "**Why this format is useful**:\n",
    "- **Easy comparison**: See 2017 vs 2018 side-by-side\n",
    "- **Heatmap ready**: Perfect structure for next visualization\n",
    "- **Pattern detection**: Scan vertically for seasonal patterns\n",
    "- **Year-over-year**: Compare horizontally\n",
    "\n",
    "**Troubleshooting note**:\n",
    "Comment mentions possible error if `prices.dateSeen` doesn't exist - this is because we already converted it to `dateSeen` in earlier cells.\n",
    "\n",
    "**Expected output**: A 12Ã—2 table showing average discount percentage for each month in 2017 and 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_dateSeen = df.set_index(\"dateSeen\").sort_index()\n",
    "\n",
    "## if you got this error \"None of ['prices.dateSeen'] are in the columns\"\n",
    "## then uncomment the above line \n",
    "plt.figure(figsize=(12, 6))\n",
    "df_pivot_table = df.pivot_table(values=\"discount_percent\", index=df_index_dateSeen.index.month, columns=df_index_dateSeen.index.year, aggfunc=\"mean\")\n",
    "print(df_pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Heatmap of Monthly Discount Patterns\n",
    "\n",
    "**Purpose**: Visualize the pivot table as a color-coded heatmap for easy pattern recognition.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "**Parameters**:\n",
    "- `df_pivot_table`: The month Ã— year data from previous cell\n",
    "- `annot=True`: Display discount percentages in each cell\n",
    "- `cmap=\"coolwarm\"`: Color map\n",
    "  - **Blue** = Lower discounts (cool colors)\n",
    "  - **Red** = Higher discounts (warm colors)\n",
    "  - **White** = Mid-range\n",
    "- `linewidths=0.5`: Thin white lines separating cells\n",
    "\n",
    "\n",
    "**Expected patterns**:\n",
    "- **Red cluster**: November-December (Black Friday, Christmas)\n",
    "- **Blue cluster**: January-February (post-holiday)\n",
    "- **Consistent columns**: Similar color progression in both years\n",
    "\n",
    "**Business insights**:\n",
    "- **Promotional calendar**: When to plan major sales\n",
    "- **Inventory planning**: Anticipate discount periods\n",
    "- **Competitive timing**: Align with industry patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(df_pivot_table, annot=True, cmap=\"coolwarm\", linewidths=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Featuring enginerring pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1 Rolling Statistics\n",
    "\n",
    "Rolling statistics (moving averages) smooth short-term fluctuations and highlight longer-term trends. Essential for:\n",
    "- Noise reduction\n",
    "- Trend identification\n",
    "- Seasonality detection\n",
    "- Preparing data for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Aggregation and Trend Visualization\n",
    "\n",
    "**Purpose**: Resample data to weekly frequency and compare discount trends across three date perspectives.\n",
    "\n",
    "**Part 1: Weekly Resampling (3 versions)**\n",
    "\n",
    "1. **Weekly average by dateSeen**:\n",
    "```python\n",
    "weekly_discount_size_seen = df.resample(\"W\", on=\"dateSeen\")[\"discount_percent\"].mean()\n",
    "```\n",
    "- `.resample(\"W\", on=\"dateSeen\")`: Groups by week based on observation date\n",
    "- `[\"discount_percent\"].mean()`: Averages discounts within each week\n",
    "- **Use**: Shows actual pricing trends seen by consumers\n",
    "\n",
    "1. **Weekly average by dateUpdated**:\n",
    "```python\n",
    "weekly_discount_size_updated = df.resample(\"W\", on=\"dateUpdated\")[\"discount_percent\"].mean()\n",
    "```\n",
    "- Groups by week when records were updated\n",
    "- **Use**: Shows data freshness patterns\n",
    "\n",
    "1. **Weekly average by dateAdded**:\n",
    "```python\n",
    "weekly_discount_size_added = df.resample(\"W\", on=\"dateAdded\")[\"discount_percent\"].mean()\n",
    "```\n",
    "- Groups by week when products entered database\n",
    "- **Use**: Shows product launch discount strategies\n",
    "\n",
    "**Why weekly frequency?**\n",
    "- **Balances detail and noise**:\n",
    "  - Daily: Too volatile\n",
    "  - Monthly: Might miss important variations\n",
    "  - Weekly: Just right for retail patterns\n",
    "\n",
    "\n",
    "**Part 2: Visualization**\n",
    "\n",
    "**What the plot shows**:\n",
    "- **Three overlapping lines**: Each representing different date perspective\n",
    "- **X-axis**: Time (weekly intervals)\n",
    "- **Y-axis**: Average discount percentage\n",
    "- **Markers**: Circle at each week's data point\n",
    "\n",
    "**How to interpret**:\n",
    "\n",
    "1. **Lines move together**: Consistent pricing across all dates\n",
    "2. **Lines diverge**: Different timing in how prices are recorded/updated\n",
    "3. **Upward trend**: Discounts increasing over time\n",
    "4. **Downward trend**: Discounts decreasing\n",
    "5. **Regular peaks/valleys**: Weekly or seasonal patterns\n",
    "6. **Sharp spikes**: Special sales events\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_discount_size_seen = df.resample(\"W\", on=\"dateSeen\")[\"discount_percent\"].mean()\n",
    "weekly_discount_size_updated = df.resample(\"W\", on=\"dateUpdated\")[\"discount_percent\"].mean()\n",
    "weekly_discount_size_added = df.resample(\"W\", on=\"dateAdded\")[\"discount_percent\"].mean()\n",
    "\n",
    "weekly_discount_size_added.head()\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.lineplot(data=weekly_discount_size_seen, label=\"Date Seen\", marker=\"o\")\n",
    "sns.lineplot(data=weekly_discount_size_updated, label=\"Date Updated\", marker=\"o\")\n",
    "sns.lineplot(data=weekly_discount_size_added, label=\"Date Added\", marker=\"o\")\n",
    "plt.title(\"Weekly Average Discount Percent (2017â€“2018)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lag Features \n",
    "\n",
    "Lag features represent previous values in a time series. They are crucial for:\n",
    "- **Autocorrelation analysis**: Understanding temporal dependencies\n",
    "- **ARIMA/SARIMA modeling**: Determining AR order\n",
    "- **Machine learning**: Using past to predict future\n",
    "- **Pattern detection**: Identifying cyclical behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Lag Features\n",
    "\n",
    "**Purpose**: Create lagged versions of the discount time series to analyze autocorrelation.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Create clean weekly time series**:\n",
    "- Sets dateSeen as index\n",
    "- Resamples to weekly frequency\n",
    "- Calculates average discount per week\n",
    "- Removes any NaN values\n",
    "- Creates clean time series `ts`\n",
    "\n",
    "2. **Initialize lag DataFrame with current values**:\n",
    "- Creates DataFrame with one column: \"current\"\n",
    "- Contains the weekly discount percentages\n",
    "- This represents time **t** (present)\n",
    "\n",
    "3. **Create Lag 1 (1 week ago)**:\n",
    "- `.shift(1)`: Moves all values down by 1 position\n",
    "- Shows last week's discount\n",
    "- First value becomes NaN (no previous week)\n",
    "\n",
    "**Why these specific lags?**\n",
    "- **Lag 1**: Immediate persistence (AR(1) component)\n",
    "- **Lag 3**: ~3-week cycles (common in retail)\n",
    "- **Lag 6**: ~1.5 month patterns\n",
    "- **Lag 12**: Quarterly cycles (~3 months)\n",
    "\n",
    "**Purpose of lag features**:\n",
    "1. **Autocorrelation**: Do past values predict future?\n",
    "2. **ARIMA order**: Which lags are significant?\n",
    "3. **Forecasting**: Use historical data as predictors\n",
    "4. **Pattern detection**: Find cyclical behavior\n",
    "\n",
    "**Expected output**: DataFrame with 5 columns showing current week and 4 lagged versions, with NaN in early rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = (df.set_index(\"dateSeen\").resample(\"W\")[\"discount_percent\"].mean()).dropna()\n",
    "lag_df = pd.DataFrame({\"current\": ts})\n",
    "\n",
    "lag_df[\"lag_1\"] = ts.shift(1)\n",
    "lag_df[\"lag_3\"] = ts.shift(3)\n",
    "lag_df[\"lag_6\"] = ts.shift(6)\n",
    "lag_df[\"lag_12\"] = ts.shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Remove Missing Values from Lag Data\n",
    "\n",
    "**Purpose**: Clean the lag DataFrame by removing rows with NaN values.\n",
    "\n",
    "\n",
    "**Why this is necessary**:\n",
    "- The shift() operation creates NaN values:\n",
    "  - Lag 1: First 1 row has NaN\n",
    "  - Lag 3: First 3 rows have NaN\n",
    "  - Lag 12: First 12 rows have NaN\n",
    "- `dropna()`: Removes rows where ANY column has NaN\n",
    "- Result: Only complete cases remain\n",
    "\n",
    "**Impact**:\n",
    "- Loses first 12 weeks of data (due to lag_12)\n",
    "- Trade-off: Smaller dataset but complete feature set\n",
    "- For ~104 weeks (2 years), ~92 usable rows remain\n",
    "\n",
    "**Why we need complete cases**:\n",
    "- Required for correlation calculations\n",
    "- Necessary for scatter plots\n",
    "- Most models require complete data\n",
    "- Ensures fair comparison across all lags\n",
    "\n",
    "**Expected output**: Cleaned lag_df with no NaN values, ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = lag_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Lag-1 Autocorrelation Plot\n",
    "\n",
    "**Purpose**: Visualize the relationship between current discount values and values from 1 week ago.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "**Components**:\n",
    "- **Square plot** (6Ã—6): Equal scales for x and y\n",
    "- **X-axis**: Last week's discount (t-1)\n",
    "- **Y-axis**: This week's discount (t)\n",
    "- **Each point**: One week of data\n",
    "\n",
    "**How to interpret patterns**:\n",
    "\n",
    "**1. Strong Positive Correlation** (upward diagonal):\n",
    "```\n",
    "  High current\n",
    "       â†‘\n",
    "       |    /\n",
    "       |   /\n",
    "       |  /\n",
    "       | /\n",
    "       |/________ High lag_1 â†’\n",
    "```\n",
    "- Points cluster along diagonal\n",
    "- High lag_1 â†’ High current\n",
    "- **Meaning**: Discounts persist week-to-week\n",
    "- **SARIMA**: Need AR(1) component\n",
    "\n",
    "**2. No Correlation** (random cloud):\n",
    "```\n",
    "       â†‘  â€¢ â€¢\n",
    "       | â€¢  â€¢ â€¢\n",
    "       |  â€¢  â€¢\n",
    "       |â€¢ â€¢   â€¢\n",
    "       |________ â†’\n",
    "```\n",
    "- Points scattered randomly\n",
    "- **Meaning**: Last week doesn't predict this week\n",
    "- **SARIMA**: Try different lags\n",
    "\n",
    "**3. Negative Correlation** (downward diagonal):\n",
    "```\n",
    "       â†‘\\\n",
    "       | \\\n",
    "       |  \\\n",
    "       |   \\\n",
    "       |    \\\n",
    "       |________ â†’\n",
    "```\n",
    "- High lag_1 â†’ Low current\n",
    "- **Meaning**: Alternating pattern\n",
    "- **Rare** in discount data\n",
    "\n",
    "**What this tells us**:\n",
    "\n",
    "- **Strong correlation**: \n",
    "  - Discounts are predictable\n",
    "  - Include AR terms in SARIMA\n",
    "  - Good 1-week-ahead forecasts possible\n",
    "\n",
    "- **Weak correlation**:\n",
    "  - Check other lags (3, 6, 12)\n",
    "  - May need seasonal components\n",
    "  - More challenging to forecast\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**: Scatter plot revealing the strength and direction of week-to-week persistence in discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(lag_df[\"lag_1\"], lag_df[\"current\"])\n",
    "plt.xlabel(\"Lag 1 (t-1)\")\n",
    "plt.ylabel(\"Current (t)\")\n",
    "plt.title(\"Lag-1 Relationship\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Next Steps\n",
    "\n",
    "### What This Notebook Accomplished:\n",
    "\n",
    "#### âœ… Data Preparation\n",
    "- Loaded and cleaned electronics pricing dataset\n",
    "- Removed 12 unnecessary columns\n",
    "- Handled missing values strategically\n",
    "- Converted dates to proper datetime format\n",
    "- Created discount percentage metric\n",
    "\n",
    "#### âœ… Feature Engineering\n",
    "- Calculated price differences and discount percentages\n",
    "- Extracted temporal features (year, month)\n",
    "- Created lag features (1, 3, 6, 12 weeks)\n",
    "- Filtered to high-quality time period (2017-2018)\n",
    "\n",
    "#### âœ… Exploratory Analysis\n",
    "- Correlation heatmap of numerical features\n",
    "- 4-panel pricing and discount dashboard\n",
    "- Seasonal pattern detection (monthly trends)\n",
    "- Year-over-year comparison\n",
    "\n",
    "#### âœ… Time Series Preparation\n",
    "- Weekly resampling for noise reduction\n",
    "- Trend visualization across three date perspectives\n",
    "- Pivot table creation (month Ã— year)\n",
    "- Seasonality heatmap\n",
    "\n",
    "#### âœ… Autocorrelation Analysis\n",
    "- Lag feature creation\n",
    "- Lag-1 scatter plot\n",
    "- Temporal dependency visualization\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Next Steps for SARIMA:\n",
    "\n",
    "1. **Stationarity Testing**:\n",
    "   - Run Augmented Dickey-Fuller test\n",
    "   - Apply differencing if needed\n",
    "   - Confirm stationarity\n",
    "\n",
    "2. **ACF/PACF Analysis**:\n",
    "   - Plot autocorrelation functions\n",
    "   - Identify p, d, q parameters\n",
    "   - Determine P, D, Q, s for seasonal component\n",
    "\n",
    "3. **Model Building**:\n",
    "   - Fit SARIMA(p,d,q)(P,D,Q,s)\n",
    "   - Test multiple parameter combinations\n",
    "   - Use AIC/BIC for model selection\n",
    "\n",
    "4. **Model Validation**:\n",
    "   - Check residuals (white noise test)\n",
    "   - Diagnostic plots\n",
    "   - Cross-validation\n",
    "\n",
    "5. **Forecasting**:\n",
    "   - Generate predictions\n",
    "   - Create confidence intervals\n",
    "   - Visualize forecasts vs actuals\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Libraries Used:\n",
    "- **pandas**: Data manipulation\n",
    "- **matplotlib**: Visualization\n",
    "- **seaborn**: Statistical plots\n",
    "- **statsmodels** (ready for next phase): SARIMA modeling\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Absolute Percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
