{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Time Series Analysis: Electronics Product Pricing Data\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive time series analysis on electronics product pricing data with the goal of understanding discount patterns, detecting seasonality, and preparing data for SARIMA forecasting.\n",
    "\n",
    "### Analysis Components:\n",
    "1. **Data Loading and Preprocessing**\n",
    "2. **Feature Engineering** (discount calculations, temporal features)\n",
    "3. **Exploratory Data Analysis** (correlations, visualizations)\n",
    "4. **Seasonality Detection** (monthly and weekly patterns)\n",
    "5. **Rolling Statistics** (weekly aggregations)\n",
    "6. **Lag Feature Engineering** (autocorrelation analysis)\n",
    "\n",
    "### Dataset:\n",
    "- **Source**: ElectronicsProductsPricingData.csv\n",
    "- **Time Period**: 2014-2018 (focus on 2017-2018)\n",
    "- **Key Metric**: Discount percentage over time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "This section imports the required libraries and loads the dataset into a pandas DataFrame. The dataset is the foundation for all subsequent preprocessing and analysis steps. We will install all the libraries that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Cell 1: Library Installation\n",
    "\n",
    "**Purpose**: Install required Python packages for time series analysis.\n",
    "\n",
    "**Libraries needed**:\n",
    "- **`statsmodels`**: Statistical models for time series (ARIMA, SARIMA, ADF test, ACF/PACF)\n",
    "- **`pandas`**: Data manipulation and analysis with DataFrames\n",
    "- **`matplotlib`**: Core plotting library for visualizations\n",
    "- **`seaborn`**: Statistical visualization built on matplotlib\n",
    "\n",
    "**Installation command**:\n",
    "```bash\n",
    "pip install statsmodels pandas matplotlib seaborn\n",
    "```\n",
    "\n",
    "**Note**: This cell is commented out because package installation should typically be done via terminal/command prompt, not within the notebook itself. Uncomment only if running in an environment where pip install in cells is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install statsmodels pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading & Saving Data\n",
    "\n",
    "### `pd.read_csv()` ‚Äî Load data\n",
    "Reads a CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Environment Setup\n",
    "We utilize pandas for data manipulation and seaborn/matplotlib for academic-standard visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ Cell 2: Load Dataset and Display Info\n",
    "\n",
    "**Purpose**: Import necessary libraries, load the electronics pricing dataset, and inspect its structure.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Import Core Libraries**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import matplotlib.pyplot as plt\n",
    "   import seaborn as sns\n",
    "   ```\n",
    "   - `pandas`: For data manipulation and analysis\n",
    "   - `matplotlib.pyplot`: For creating plots and visualizations\n",
    "   - `seaborn`: For statistical visualizations and attractive default styles\n",
    "\n",
    "2. **Load CSV File**:\n",
    "   ```python\n",
    "   df = pd.read_csv(\"./ElectronicsProductsPricingData.csv\", encoding='latin1')\n",
    "   ```\n",
    "   - Reads the CSV file from the current directory\n",
    "   - `encoding='latin1'`: Handles special characters in product names (accents, symbols)\n",
    "   - Stores data in DataFrame `df`\n",
    "\n",
    "3. **Display Dataset Information**:\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "   - Shows comprehensive dataset summary:\n",
    "     - Number of rows and columns\n",
    "     - Column names and their data types\n",
    "     - Non-null counts (identifies missing values)\n",
    "     - Memory usage\n",
    "\n",
    "**Expected Output**:\n",
    "- DataFrame index range\n",
    "- List of all columns with their types (int64, float64, object)\n",
    "- Count of non-null values per column\n",
    "- Total memory usage\n",
    "\n",
    "**Key columns to note**:\n",
    "- Price-related: `prices.amountMin`, `prices.amountMax`, `prices.isSale`\n",
    "- Date-related: `prices.dateSeen`, `dateAdded`, `dateUpdated`\n",
    "- Product info: `id`, `brand`, `categories`, `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./ElectronicsProductsPricingData.csv\", encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßπ Cell 3: Remove Unnecessary Columns\n",
    "\n",
    "**Purpose**: Clean the dataset by dropping columns that are not needed for price and discount analysis.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "```python\n",
    "df.drop(columns=[...], inplace=True)\n",
    "df.head()\n",
    "```\n",
    "\n",
    "**Columns being removed**:\n",
    "\n",
    "1. **`Unnamed: 26-30`**: Empty columns likely created during data export\n",
    "2. **`sourceURLs`**: Web source links (not needed for analysis)\n",
    "3. **`prices.currency`**: Currency type (assuming all same currency)\n",
    "4. **`keys`**: Internal database keys\n",
    "5. **`ean`**: European Article Number (barcode)\n",
    "6. **`prices.shipping`**: Shipping costs (focusing on product price only)\n",
    "7. **`manufacturerNumber`**: Manufacturer-specific product codes\n",
    "8. **`upc`**: Universal Product Code (another barcode standard)\n",
    "\n",
    "**Parameters**:\n",
    "- `inplace=True`: Modifies the original DataFrame instead of creating a copy\n",
    "\n",
    "**Why remove these columns**:\n",
    "- **Reduces memory**: Smaller dataset is faster to process\n",
    "- **Improves clarity**: Easier to see relevant columns\n",
    "- **Focuses analysis**: Only keeps features needed for time series modeling\n",
    "\n",
    "**Output**:\n",
    "- `df.head()` displays first 5 rows of the cleaned dataset\n",
    "- Verify that unwanted columns are gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing all the unnecessary columns from the dataframe\n",
    "df.drop(columns=[\"Unnamed: 26\",\"Unnamed: 27\",\"Unnamed: 28\",\"Unnamed: 29\",\"Unnamed: 30\",\"sourceURLs\",\"prices.currency\", \"keys\",\"ean\",\"prices.shipping\",\"manufacturerNumber\",\"upc\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí∞ Cell 4: Calculate Price Difference and Discount Percentage\n",
    "\n",
    "**Purpose**: Create new features to quantify discounts for each product.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Remove rows with missing prices**:\n",
    "   ```python\n",
    "   df.dropna(subset=[\"prices.amountMax\", \"prices.amountMin\"], inplace=True)\n",
    "   ```\n",
    "   - Drops rows where either max or min price is NaN\n",
    "   - Ensures we can calculate differences without errors\n",
    "\n",
    "2. **Calculate absolute price difference**:\n",
    "   ```python\n",
    "   df[\"price_difference\"] = df[\"prices.amountMax\"] - df[\"prices.amountMin\"]\n",
    "   ```\n",
    "   - Formula: `difference = max_price - min_price`\n",
    "   - Shows discount in currency units (e.g., $20 off)\n",
    "\n",
    "3. **Calculate discount percentage**:\n",
    "   ```python\n",
    "   df['discount_percent'] = (df['price_difference'] / df['prices.amountMax']) * 100\n",
    "   ```\n",
    "   - Formula: `discount% = (difference / max_price) √ó 100`\n",
    "   - **Example**: Max=$100, Min=$80\n",
    "     - difference = $20\n",
    "     - discount% = (20/100) √ó 100 = 20%\n",
    "   - **Advantage**: Standardized metric allowing comparison across price ranges\n",
    "\n",
    "4. **Display updated info**:\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "   - Shows new columns added\n",
    "   - Confirms number of rows after dropping nulls\n",
    "\n",
    "**Why discount percentage matters**:\n",
    "- **Time series target**: This is the primary metric we'll forecast\n",
    "- **Comparability**: 20% discount means the same whether on $10 or $1000 item\n",
    "- **Business relevance**: Reflects actual promotional strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##1.2 we will check if the prices amountMax and minAmount is same \n",
    "## we have also put a new column discount percentage.\n",
    "\n",
    "df.dropna(subset=[\"prices.amountMax\", \"prices.amountMin\"], inplace=True)\n",
    "df[\"price_difference\"] = df[\"prices.amountMax\"] - df[\"prices.amountMin\"]\n",
    "df['discount_percent'] = (df['price_difference'] / df['prices.amountMax']) * 100\n",
    " \n",
    "df.info()\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Cell 5: Analyze Product ID Distribution\n",
    "\n",
    "**Purpose**: Identify which products have multiple price entries (good candidates for time series analysis).\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "```python\n",
    "df[\"id\"].value_counts()\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- `value_counts()`: Counts occurrences of each unique product ID\n",
    "- Returns a Series sorted by frequency (descending)\n",
    "- Shows which products appear most often in the dataset\n",
    "\n",
    "**What the output tells us**:\n",
    "\n",
    "- **High counts** (e.g., 150+ entries):\n",
    "  - Product with extensive price history\n",
    "  - Multiple observations over time\n",
    "  - **Excellent for time series modeling**\n",
    "  - Can detect trends and seasonality\n",
    "\n",
    "- **Medium counts** (e.g., 20-50 entries):\n",
    "  - Reasonable historical data\n",
    "  - Sufficient for basic trend analysis\n",
    "\n",
    "- **Low counts** (e.g., 1-5 entries):\n",
    "  - Limited history\n",
    "  - Not suitable for time series modeling\n",
    "  - May be new products or single observations\n",
    "\n",
    "**Why this matters**:\n",
    "- **Product selection**: Helps choose which products to analyze in detail\n",
    "- **Data richness**: Products with more entries provide better forecasts\n",
    "- **Time series viability**: Need sufficient data points for SARIMA models\n",
    "\n",
    "**Next step**: Select a high-frequency product ID for detailed case study (done in next cell)\n",
    "\n",
    "**Expected output format**:\n",
    "```\n",
    "AV1YFZVDvKc47QAVgp7V    150\n",
    "AV2XGHJKlmn48BZWhr8Q     89\n",
    "AV3PLMNOpqr49CXYis9R     45\n",
    "...\n",
    "Name: id, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will check if the data has unique ids \n",
    "df[\"id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Cell 6: Filter for Specific Product Case Study\n",
    "\n",
    "**Purpose**: Focus on one product with rich historical data and filter for sale periods.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Filter by specific product ID**:\n",
    "   ```python\n",
    "   df_id = df[df[\"id\"] == \"AV1YFZVDvKc47QAVgp7V\"]\n",
    "   ```\n",
    "   - Selects all rows for this particular product\n",
    "   - This ID was likely found to have many entries in previous cell\n",
    "   - Creates a single-product time series\n",
    "\n",
    "2. **Filter for sale items only**:\n",
    "   ```python\n",
    "   df_id = df_id[df_id[\"prices.isSale\"] == True]\n",
    "   ```\n",
    "   - Keeps only records where product was actively on sale\n",
    "   - `prices.isSale`: Boolean column indicating promotional status\n",
    "   - Focuses on discount periods specifically\n",
    "\n",
    "3. **Display results**:\n",
    "   ```python\n",
    "   df_id.head()\n",
    "   ```\n",
    "   - Shows first 5 sale records for this product\n",
    "\n",
    "**Why this approach**:\n",
    "\n",
    "- **Case study methodology**: Deep dive into one product before generalizing\n",
    "- **Data richness**: This product has sufficient history for analysis\n",
    "- **Sale focus**: Understand promotional pricing strategy specifically\n",
    "- **Simpler modeling**: Single product is easier than multi-product aggregation\n",
    "\n",
    "**What you can learn from this**:\n",
    "- How one product's discount changes over time\n",
    "- Seasonal patterns in promotional pricing\n",
    "- Frequency of sales events\n",
    "- Magnitude of typical discounts\n",
    "\n",
    "**Potential uses**:\n",
    "- Build product-specific SARIMA model\n",
    "- Identify optimal sale timing\n",
    "- Forecast future discount periods\n",
    "- Compare against category averages\n",
    "\n",
    "**Expected output**: Subset of DataFrame showing only sale records for product ID \"AV1YFZVDvKc47QAVgp7V\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking in the dataframe electronics if there are different products\n",
    "## from the above category, we have checked if the dataframe has different ids, and then checked how many values each id has, where we found one particular interested one. \n",
    "\n",
    "\n",
    "df_id = df[df[\"id\"] == \"AV1YFZVDvKc47QAVgp7V\"]\n",
    "df_id = df_id[df_id[\"prices.isSale\"] == True]\n",
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Format and Discount Calculation\n",
    "\n",
    "Proper date handling is crucial for time series analysis. This section converts string dates to datetime objects and extracts temporal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÖ Cell 7: Date Parsing and Temporal Feature Engineering\n",
    "\n",
    "**Purpose**: Convert date strings to datetime objects and extract year components for temporal analysis.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Re-import libraries** (ensuring they're loaded):\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import matplotlib.pyplot as plt\n",
    "   import seaborn as sns\n",
    "   ```\n",
    "\n",
    "2. **Convert three date columns to datetime format**:\n",
    "   ```python\n",
    "   df[\"dateSeen\"] = pd.to_datetime(df[\"prices.dateSeen\"], errors=\"coerce\")\n",
    "   df[\"dateUpdated\"] = pd.to_datetime(df[\"dateUpdated\"], errors=\"coerce\")\n",
    "   df[\"dateAdded\"] = pd.to_datetime(df[\"dateAdded\"], errors=\"coerce\")\n",
    "   ```\n",
    "   \n",
    "   **Each date field serves a different purpose**:\n",
    "   - **`dateSeen`**: When the price was actually observed/scraped\n",
    "     - Most relevant for understanding consumer-facing prices\n",
    "     - Best for analyzing real-world pricing trends\n",
    "   \n",
    "   - **`dateUpdated`**: When the database record was last modified\n",
    "     - Shows data freshness\n",
    "     - May lag behind actual price changes\n",
    "   \n",
    "   - **`dateAdded`**: When product first entered the database\n",
    "     - Useful for understanding product lifecycle\n",
    "     - Initial pricing analysis\n",
    "   \n",
    "   **`errors=\"coerce\"`**: Invalid dates become NaT (Not a Time) instead of raising errors\n",
    "\n",
    "3. **Extract year from each datetime column**:\n",
    "   ```python\n",
    "   df[\"date_year\"] = df[\"dateSeen\"].dt.year\n",
    "   df[\"updated_year\"] = df[\"dateUpdated\"].dt.year\n",
    "   df[\"added_year\"] = df[\"dateAdded\"].dt.year\n",
    "   ```\n",
    "   - `.dt.year`: Accessor for datetime year component\n",
    "   - Creates integer columns (2014, 2015, 2016, etc.)\n",
    "   - Enables easy year-based filtering and grouping\n",
    "\n",
    "4. **Check year distribution**:\n",
    "   ```python\n",
    "   df[\"date_year\"].value_counts()\n",
    "   ```\n",
    "   - Shows how many records per year\n",
    "   - Identifies temporal coverage of dataset\n",
    "   - Helps decide which years to focus on\n",
    "\n",
    "5. **Data validation - remove unparseable dates**:\n",
    "   ```python\n",
    "   df = df.dropna(subset=['dateAdded', 'dateUpdated'])\n",
    "   ```\n",
    "   - Removes rows where date conversion failed\n",
    "   - Ensures clean datetime data for all remaining records\n",
    "   - Part of data validation pipeline\n",
    "\n",
    "6. **Remove redundant original column**:\n",
    "   ```python\n",
    "   df.drop(columns=['prices.dateSeen'], inplace=True)\n",
    "   ```\n",
    "   - The original `prices.dateSeen` is now replaced by `dateSeen`\n",
    "   - Reduces memory and eliminates confusion\n",
    "\n",
    "7. **Display cleaned data**:\n",
    "   ```python\n",
    "   df.head()\n",
    "   ```\n",
    "\n",
    "**Why datetime conversion matters**:\n",
    "- **Enables time-based operations**:\n",
    "  - Sorting chronologically\n",
    "  - Resampling (daily ‚Üí weekly ‚Üí monthly)\n",
    "  - Date filtering and slicing\n",
    "  - Time difference calculations\n",
    "\n",
    "- **Unlocks pandas datetime methods**:\n",
    "  - Extract month, day, quarter\n",
    "  - Day of week analysis\n",
    "  - Holiday detection\n",
    "  - Time series indexing\n",
    "\n",
    "**Expected output**:\n",
    "- Value counts showing year distribution (e.g., 2017: 5000 records, 2018: 4500 records)\n",
    "- DataFrame with new datetime and year columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## converting the date columns (which are in strings) to datetime format\n",
    "df[\"dateSeen\"] = pd.to_datetime(df[\"prices.dateSeen\"], errors=\"coerce\")\n",
    "df[\"dateUpdated\"] = pd.to_datetime(df[\"dateUpdated\"], errors=\"coerce\")\n",
    "df[\"dateAdded\"] = pd.to_datetime(df[\"dateAdded\"], errors=\"coerce\")\n",
    "\n",
    "## extracting the year from the date column\n",
    "df[\"date_year\"] = df[\"dateSeen\"].dt.year\n",
    "df[\"updated_year\"] = df[\"dateUpdated\"].dt.year\n",
    "df[\"added_year\"] = df[\"dateAdded\"].dt.year\n",
    "\n",
    "df[\"date_year\"].value_counts()\n",
    "# 2. Drop rows where dates couldn't be parsed (Cleaning the dataset)\n",
    "# This is part of your \"Data validation pipeline\" milestone \n",
    "df = df.dropna(subset=['dateAdded', 'dateUpdated'])\n",
    "\n",
    "## we delete the column prices.dateSeen \n",
    "df.drop(columns=['prices.dateSeen'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Empty Workspace Cell\n",
    "\n",
    "This cell is intentionally empty - commonly used in notebooks for:\n",
    "- Quick code testing\n",
    "- Temporary calculations\n",
    "- Visual separation between sections\n",
    "- Future code additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî• Cell 9: Correlation Heatmap\n",
    "\n",
    "**Purpose**: Visualize correlations between all numerical variables in the dataset.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Select only numerical columns**:\n",
    "   ```python\n",
    "   df_heat = df.select_dtypes(include=['float64', 'int64'])\n",
    "   ```\n",
    "   - Filters DataFrame to keep only numeric data types\n",
    "   - Excludes strings, dates, and boolean columns\n",
    "   - Necessary because correlation requires numerical data\n",
    "\n",
    "2. **Calculate correlation matrix**:\n",
    "   ```python\n",
    "   df_heat.corr()\n",
    "   ```\n",
    "   - Computes Pearson correlation between all numeric pairs\n",
    "   - Values range from -1 to +1\n",
    "\n",
    "3. **Create heatmap visualization**:\n",
    "   ```python\n",
    "   plt.figure(figsize=(6, 4))\n",
    "   sns.heatmap(df_heat.corr(), annot=True, cmap=\"coolwarm\")\n",
    "   ```\n",
    "   - `annot=True`: Shows correlation values in each cell\n",
    "   - `cmap=\"coolwarm\"`: Blue (negative) ‚Üí White (zero) ‚Üí Red (positive)\n",
    "\n",
    "4. **Add title and display**:\n",
    "   ```python\n",
    "   plt.title(\"Electronics Products Pricing Data Correlation Heatmap\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "**How to read the heatmap**:\n",
    "- **Dark red** (~0.7 to 1.0): Strong positive correlation\n",
    "- **Light red/pink** (~0.3 to 0.7): Moderate positive correlation\n",
    "- **White** (~-0.3 to 0.3): Little to no correlation\n",
    "- **Light blue** (~-0.7 to -0.3): Moderate negative correlation\n",
    "- **Dark blue** (~-1.0 to -0.7): Strong negative correlation\n",
    "\n",
    "**Expected correlations**:\n",
    "- **prices.amountMin ‚Üî prices.amountMax**: Very high positive (0.9+)\n",
    "- **price_difference ‚Üî prices.amountMax**: Positive (larger max = larger difference)\n",
    "- **discount_percent ‚Üî price_difference**: Positive correlation\n",
    "\n",
    "**Why this matters for time series**:\n",
    "- Identifies multicollinearity issues\n",
    "- Helps select independent variables\n",
    "- Reveals hidden relationships\n",
    "- Guides feature engineering decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will use the heatmap for the dataset to visualize the correlation between different numerical features\n",
    "## For that we will use seaborn library\n",
    "## Also we will filter out the non-numerical columns from the dataset\n",
    "\n",
    "\n",
    "df_heat = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Electronics Products Pricing Data heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(df_heat.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Electronics Products Pricing Data Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Visualization \n",
    "\n",
    "Comprehensive visualizations to understand pricing patterns, discount trends, and temporal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Exploratory Data Analysis: Multi-Dimensional Discount Analysis\n",
    "To identify price volatility and seasonal discounting cycles, we implemented a 2x2 visualization matrix. This aligns with the Week 3-4 Milestone of performing deep exploratory data analysis (EDA) to justify the transition to predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2x2 frame\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('DealCatcher EDA: Price & Discount Trends', fontsize=20)\n",
    "\n",
    "# 1. Scatterplot of Min-Max Prices\n",
    "sns.scatterplot(ax=axes[0, 0], data=df, x='prices.amountMin', y='prices.amountMax', alpha=0.5)\n",
    "axes[0, 0].set_title('Scatterplot of Min-Max Prices')\n",
    "\n",
    "# 2. Discount% vs Date Added\n",
    "sns.scatterplot(ax=axes[0, 1], data=df, x='dateAdded', y='discount_percent', color='orange')\n",
    "axes[0, 1].set_title('Discount % vs Date Added')\n",
    "\n",
    "# 3. Discount% vs Date Last Seen\n",
    "sns.scatterplot(ax=axes[1, 0], data=df, x='dateSeen', y='discount_percent', color='green')\n",
    "axes[1, 0].set_title('Discount % vs Date Seen')\n",
    "\n",
    "# 4. Discount% vs Date Updated\n",
    "sns.scatterplot(ax=axes[1, 1], data=df, x='dateUpdated', y='discount_percent', color='red')\n",
    "axes[1, 1].set_title('Discount % vs Date Updated')\n",
    "\n",
    "# Rotate labels so they don't overlap\n",
    "for ax in axes.flat:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Cell 11: Price Range Visualization (Contains Bug)\n",
    "\n",
    "**Purpose**: Attempt to visualize price ranges over DataFrame indices.\n",
    "\n",
    "**What this code TRIES to do**:\n",
    "- Plot maximum and minimum prices over the dataset index\n",
    "\n",
    "**üêõ BUG ALERT**:\n",
    "```python\n",
    "plt.plot(df.index, df[\"dateUpdated\"], marker=\"x\", label=\"Amount Min\")\n",
    "```\n",
    "This line plots `dateUpdated` (dates!) instead of `prices.amountMin` (prices)!\n",
    "\n",
    "**What it SHOULD be**:\n",
    "```python\n",
    "plt.plot(df.index, df[\"prices.amountMin\"], marker=\"x\", label=\"Amount Min\")\n",
    "```\n",
    "\n",
    "**Better alternative** - plot by actual date:\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"dateUpdated\"], df[\"prices.amountMax\"], marker=\"o\", label=\"Amount Max\")\n",
    "plt.plot(df[\"dateUpdated\"], df[\"prices.amountMin\"], marker=\"x\", label=\"Amount Min\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Price Range Over Time\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Note**: This cell has a coding error that should be fixed before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Observations & Analytical  Insights\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[\"dateUpdated\"] = pd.to_datetime(df[\"dateUpdated\"], errors='coerce')\n",
    "plt.plot(\n",
    "    df.index,\n",
    "    df[\"prices.amountMax\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Amount Max\"\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    df.index,\n",
    "    df[\"dateUpdated\"],\n",
    "    marker=\"x\",\n",
    "    label=\"Amount Min\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Price Range per Product\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Cell 12: Seasonal Discount Analysis (2017-2018)\n",
    "\n",
    "**Purpose**: Filter data to 2017-2018 and create two plots showing seasonal discount patterns from different date perspectives.\n",
    "\n",
    "**Part 1: Data Filtering**\n",
    "\n",
    "```python\n",
    "df = df[df[\"updated_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"date_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"added_year\"].isin([2017, 2018])]\n",
    "```\n",
    "\n",
    "**Why filter to 2017-2018?**\n",
    "- Earlier analysis showed 2014-2015 had sparse data\n",
    "- 2017-2018 have the highest data density\n",
    "- Ensures all three date fields are within range\n",
    "- Focuses on most recent and complete period\n",
    "\n",
    "**Part 2: Plot 1 - Seasonality by Date Seen**\n",
    "\n",
    "```python\n",
    "df[\"month\"] = df[\"dateSeen\"].dt.month\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x=\"month\", y=\"discount_percent\", hue=\"date_year\", marker=\"o\")\n",
    "```\n",
    "\n",
    "**What this does**:\n",
    "- Extracts month (1-12) from dateSeen\n",
    "- Creates line plot with:\n",
    "  - X-axis: Months (1=Jan, 12=Dec)\n",
    "  - Y-axis: Average discount percentage\n",
    "  - Two lines (one for 2017, one for 2018)\n",
    "  - `marker=\"o\"`: Circle at each month\n",
    "\n",
    "**How to interpret**:\n",
    "- **Peaks**: Months with highest discounts (likely Nov-Dec)\n",
    "- **Valleys**: Months with lowest discounts (likely Jan-Feb)\n",
    "- **Line comparison**: Year-over-year consistency\n",
    "\n",
    "**Part 3: Plot 2 - Seasonality by Date Added**\n",
    "\n",
    "```python\n",
    "df_added = df.copy()\n",
    "df_added[\"month\"] = df_added[\"dateAdded\"].dt.month\n",
    "sns.lineplot(data=df_added, x=\"month\", y=\"discount_percent\", hue=\"added_year\", marker=\"o\")\n",
    "```\n",
    "\n",
    "**Why create a copy**:\n",
    "- `df.copy()`: Prevents modifying original DataFrame\n",
    "- Allows independent month extraction from different date field\n",
    "\n",
    "**What this shows**:\n",
    "- Discount patterns based on when products were added\n",
    "- Different perspective from observation dates\n",
    "- May reveal product launch strategies\n",
    "\n",
    "**Title note**: Says \"Date Updated\" but actually uses \"Date Added\" (minor labeling error)\n",
    "\n",
    "**Business insights**:\n",
    "- **Holiday spikes**: November-December typically highest\n",
    "- **Post-holiday dips**: January-February typically lowest\n",
    "- **Mid-year sales**: Possible June-July bumps\n",
    "- **Consistency**: Do patterns repeat year-over-year?\n",
    "\n",
    "**SARIMA implications**:\n",
    "- Clear seasonality supports S component\n",
    "- Seasonal period = 12 (monthly cycle)\n",
    "- Regular patterns improve forecast accuracy\n",
    "\n",
    "**Expected output**: Two line plots showing monthly discount percentage trends with year-over-year comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove the years 2014 and 2015 from the dataset as they have very few data points and we will focus on the years 2017 and 2018 for our seasonality analysis.\n",
    "\n",
    "\n",
    "## here we will change the data from the electronics dataset to only include the years 2017 and 2018 for the date seen, date updated and date added columns as they have more data points and we will focus on these years for our seasonality analysis.\n",
    "df = df[df[\"updated_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"date_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"added_year\"].isin([2017, 2018])]\n",
    "\n",
    "df[\"month\"] = df[\"dateSeen\"].dt.month\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(data=df, x=\"month\", y=\"discount_percent\", hue=\"date_year\", marker=\"o\")\n",
    "plt.title(\"Discount Seasonality using Date Seen (2017‚Äì2018)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##\n",
    "df_added = df.copy()\n",
    "df_added[\"month\"] = df_added[\"dateAdded\"].dt.month\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=df_added,\n",
    "    x=\"month\",\n",
    "    y=\"discount_percent\",\n",
    "    hue=\"added_year\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "plt.title(\"Discount Seasonality using Date Updated (2017‚Äì2018)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Discount Percent\")\n",
    "plt.legend(title=\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóìÔ∏è Cell 13: Create Month √ó Year Pivot Table\n",
    "\n",
    "**Purpose**: Reorganize data into a pivot table with months as rows and years as columns.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Set dateSeen as index and sort chronologically**:\n",
    "```python\n",
    "df_index_dateSeen = df.set_index(\"dateSeen\").sort_index()\n",
    "```\n",
    "- Makes dateSeen the DataFrame index\n",
    "- `.sort_index()`: Sorts rows by date (earliest to latest)\n",
    "- Prepares data for time-series operations\n",
    "\n",
    "2. **Create pivot table**:\n",
    "```python\n",
    "df_pivot_table = df.pivot_table(\n",
    "    values=\"discount_percent\",\n",
    "    index=df_index_dateSeen.index.month,\n",
    "    columns=df_index_dateSeen.index.year,\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Parameters explained**:\n",
    "- `values=\"discount_percent\"`: The metric to aggregate\n",
    "- `index=...index.month`: Rows = months (1-12)\n",
    "- `columns=...index.year`: Columns = years (2017, 2018)\n",
    "- `aggfunc=\"mean\"`: Calculate average discount per cell\n",
    "\n",
    "**Resulting structure**:\n",
    "```\n",
    "        2017    2018\n",
    "1       15.2    16.8\n",
    "2       14.5    15.3\n",
    "3       18.9    19.2\n",
    "...\n",
    "12      25.4    26.1\n",
    "```\n",
    "\n",
    "**Why this format is useful**:\n",
    "- **Easy comparison**: See 2017 vs 2018 side-by-side\n",
    "- **Heatmap ready**: Perfect structure for next visualization\n",
    "- **Pattern detection**: Scan vertically for seasonal patterns\n",
    "- **Year-over-year**: Compare horizontally\n",
    "\n",
    "**Troubleshooting note**:\n",
    "Comment mentions possible error if `prices.dateSeen` doesn't exist - this is because we already converted it to `dateSeen` in earlier cells.\n",
    "\n",
    "**Expected output**: A 12√ó2 table showing average discount percentage for each month in 2017 and 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_dateSeen = df.set_index(\"dateSeen\").sort_index()\n",
    "\n",
    "## if you got this error \"None of ['prices.dateSeen'] are in the columns\"\n",
    "## then uncomment the above line \n",
    "plt.figure(figsize=(12, 6))\n",
    "df_pivot_table = df.pivot_table(values=\"discount_percent\", index=df_index_dateSeen.index.month, columns=df_index_dateSeen.index.year, aggfunc=\"mean\")\n",
    "print(df_pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé® Cell 14: Heatmap of Monthly Discount Patterns\n",
    "\n",
    "**Purpose**: Visualize the pivot table as a color-coded heatmap for easy pattern recognition.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "```python\n",
    "sns.heatmap(df_pivot_table, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "```\n",
    "\n",
    "**Parameters**:\n",
    "- `df_pivot_table`: The month √ó year data from previous cell\n",
    "- `annot=True`: Display discount percentages in each cell\n",
    "- `cmap=\"coolwarm\"`: Color map\n",
    "  - **Blue** = Lower discounts (cool colors)\n",
    "  - **Red** = Higher discounts (warm colors)\n",
    "  - **White** = Mid-range\n",
    "- `linewidths=0.5`: Thin white lines separating cells\n",
    "\n",
    "**How to read this heatmap**:\n",
    "\n",
    "1. **Vertical scanning** (down columns):\n",
    "   - Shows seasonal pattern within each year\n",
    "   - Example: Both 2017 and 2018 columns show red in December\n",
    "\n",
    "2. **Horizontal scanning** (across rows):\n",
    "   - Compares same month across years\n",
    "   - Example: Is December 2017 similar to December 2018?\n",
    "\n",
    "3. **Color intensity**:\n",
    "   - **Darkest red**: Highest discounts (holiday season)\n",
    "   - **Darkest blue**: Lowest discounts (post-holiday)\n",
    "   - **Gradient**: Gradual changes month-to-month\n",
    "\n",
    "**Expected patterns**:\n",
    "- **Red cluster**: November-December (Black Friday, Christmas)\n",
    "- **Blue cluster**: January-February (post-holiday)\n",
    "- **Consistent columns**: Similar color progression in both years\n",
    "\n",
    "**Business insights**:\n",
    "- **Promotional calendar**: When to plan major sales\n",
    "- **Inventory planning**: Anticipate discount periods\n",
    "- **Competitive timing**: Align with industry patterns\n",
    "\n",
    "**SARIMA modeling value**:\n",
    "- Visual confirmation of seasonality\n",
    "- Helps validate seasonal period (s=12)\n",
    "- Shows consistency needed for forecasting\n",
    "\n",
    "**Expected output**: Color-coded 12√ó2 grid showing discount intensity by month and year, with numerical values displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(df_pivot_table, annot=True, cmap=\"coolwarm\", linewidths=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling Statistics\n",
    "\n",
    "Rolling statistics (moving averages) smooth short-term fluctuations and highlight longer-term trends. Essential for:\n",
    "- Noise reduction\n",
    "- Trend identification\n",
    "- Seasonality detection\n",
    "- Preparing data for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Cell 15: Weekly Aggregation and Trend Visualization\n",
    "\n",
    "**Purpose**: Resample data to weekly frequency and compare discount trends across three date perspectives.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "**Part 1: Weekly Resampling (3 versions)**\n",
    "\n",
    "1. **Weekly average by dateSeen**:\n",
    "```python\n",
    "weekly_discount_size_seen = df.resample(\"W\", on=\"dateSeen\")[\"discount_percent\"].mean()\n",
    "```\n",
    "- `.resample(\"W\", on=\"dateSeen\")`: Groups by week based on observation date\n",
    "- `[\"discount_percent\"].mean()`: Averages discounts within each week\n",
    "- **Use**: Shows actual pricing trends seen by consumers\n",
    "\n",
    "2. **Weekly average by dateUpdated**:\n",
    "```python\n",
    "weekly_discount_size_updated = df.resample(\"W\", on=\"dateUpdated\")[\"discount_percent\"].mean()\n",
    "```\n",
    "- Groups by week when records were updated\n",
    "- **Use**: Shows data freshness patterns\n",
    "\n",
    "3. **Weekly average by dateAdded**:\n",
    "```python\n",
    "weekly_discount_size_added = df.resample(\"W\", on=\"dateAdded\")[\"discount_percent\"].mean()\n",
    "```\n",
    "- Groups by week when products entered database\n",
    "- **Use**: Shows product launch discount strategies\n",
    "\n",
    "**Why weekly frequency?**\n",
    "- **Balances detail and noise**:\n",
    "  - Daily: Too volatile\n",
    "  - Monthly: Might miss important variations\n",
    "  - Weekly: Just right for retail patterns\n",
    "- **Business relevance**: Many sales run weekly\n",
    "- **Smoother visualization**: Clearer trends\n",
    "\n",
    "**Part 2: Visualization**\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=weekly_discount_size_seen, label=\"Date Seen\", marker=\"o\")\n",
    "sns.lineplot(data=weekly_discount_size_updated, label=\"Date Updated\", marker=\"o\")\n",
    "sns.lineplot(data=weekly_discount_size_added, label=\"Date Added\", marker=\"o\")\n",
    "plt.title(\"Weekly Average Discount Percent (2017‚Äì2018)\")\n",
    "```\n",
    "\n",
    "**What the plot shows**:\n",
    "- **Three overlapping lines**: Each representing different date perspective\n",
    "- **X-axis**: Time (weekly intervals)\n",
    "- **Y-axis**: Average discount percentage\n",
    "- **Markers**: Circle at each week's data point\n",
    "\n",
    "**How to interpret**:\n",
    "\n",
    "1. **Lines move together**: Consistent pricing across all dates\n",
    "2. **Lines diverge**: Different timing in how prices are recorded/updated\n",
    "3. **Upward trend**: Discounts increasing over time\n",
    "4. **Downward trend**: Discounts decreasing\n",
    "5. **Regular peaks/valleys**: Weekly or seasonal patterns\n",
    "6. **Sharp spikes**: Special sales events\n",
    "\n",
    "**SARIMA preparation**:\n",
    "- Weekly data often better for modeling than daily\n",
    "- Reveals clear trend components\n",
    "- Shows seasonal patterns more clearly\n",
    "- Identifies appropriate lag structures\n",
    "\n",
    "**Expected output**:\n",
    "- First 5 weekly averages printed\n",
    "- Line plot showing weekly discount trends from three perspectives across 2017-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_discount_size_seen = df.resample(\"W\", on=\"dateSeen\")[\"discount_percent\"].mean()\n",
    "weekly_discount_size_updated = df.resample(\"W\", on=\"dateUpdated\")[\"discount_percent\"].mean()\n",
    "weekly_discount_size_added = df.resample(\"W\", on=\"dateAdded\")[\"discount_percent\"].mean()\n",
    "\n",
    "weekly_discount_size_added.head()\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.lineplot(data=weekly_discount_size_seen, label=\"Date Seen\", marker=\"o\")\n",
    "sns.lineplot(data=weekly_discount_size_updated, label=\"Date Updated\", marker=\"o\")\n",
    "sns.lineplot(data=weekly_discount_size_added, label=\"Date Added\", marker=\"o\")\n",
    "plt.title(\"Weekly Average Discount Percent (2017‚Äì2018)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features \n",
    "\n",
    "Lag features represent previous values in a time series. They are crucial for:\n",
    "- **Autocorrelation analysis**: Understanding temporal dependencies\n",
    "- **ARIMA/SARIMA modeling**: Determining AR order\n",
    "- **Machine learning**: Using past to predict future\n",
    "- **Pattern detection**: Identifying cyclical behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∞ Cell 17: Creating Lag Features\n",
    "\n",
    "**Purpose**: Create lagged versions of the discount time series to analyze autocorrelation.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Create clean weekly time series**:\n",
    "```python\n",
    "ts = (df.set_index(\"dateSeen\").resample(\"W\")[\"discount_percent\"].mean()).dropna()\n",
    "```\n",
    "- Sets dateSeen as index\n",
    "- Resamples to weekly frequency\n",
    "- Calculates average discount per week\n",
    "- Removes any NaN values\n",
    "- Creates clean time series `ts`\n",
    "\n",
    "2. **Initialize lag DataFrame with current values**:\n",
    "```python\n",
    "lag_df = pd.DataFrame({\"current\": ts})\n",
    "```\n",
    "- Creates DataFrame with one column: \"current\"\n",
    "- Contains the weekly discount percentages\n",
    "- This represents time **t** (present)\n",
    "\n",
    "3. **Create Lag 1 (1 week ago)**:\n",
    "```python\n",
    "lag_df[\"lag_1\"] = ts.shift(1)\n",
    "```\n",
    "- `.shift(1)`: Moves all values down by 1 position\n",
    "- Shows last week's discount\n",
    "- First value becomes NaN (no previous week)\n",
    "\n",
    "4. **Create Lag 3 (3 weeks ago)**:\n",
    "```python\n",
    "lag_df[\"lag_3\"] = ts.shift(3)\n",
    "```\n",
    "- Shifts by 3 positions\n",
    "- Captures short-term cycles\n",
    "\n",
    "5. **Create Lag 6 (6 weeks ago)**:\n",
    "```python\n",
    "lag_df[\"lag_6\"] = ts.shift(6)\n",
    "```\n",
    "- Shifts by 6 positions (~1.5 months)\n",
    "- Medium-term patterns\n",
    "\n",
    "6. **Create Lag 12 (12 weeks ago)**:\n",
    "```python\n",
    "lag_df[\"lag_12\"] = ts.shift(12)\n",
    "```\n",
    "- Shifts by 12 positions (~3 months)\n",
    "- Quarterly patterns\n",
    "\n",
    "**Example of how shift() works**:\n",
    "```\n",
    "Date         current  lag_1  lag_3\n",
    "2017-01-01   15.2     NaN    NaN\n",
    "2017-01-08   16.8     15.2   NaN\n",
    "2017-01-15   14.3     16.8   NaN\n",
    "2017-01-22   18.5     14.3   NaN\n",
    "2017-01-29   17.2     18.5   15.2\n",
    "```\n",
    "\n",
    "**Why these specific lags?**\n",
    "- **Lag 1**: Immediate persistence (AR(1) component)\n",
    "- **Lag 3**: ~3-week cycles (common in retail)\n",
    "- **Lag 6**: ~1.5 month patterns\n",
    "- **Lag 12**: Quarterly cycles (~3 months)\n",
    "\n",
    "**Purpose of lag features**:\n",
    "1. **Autocorrelation**: Do past values predict future?\n",
    "2. **ARIMA order**: Which lags are significant?\n",
    "3. **Forecasting**: Use historical data as predictors\n",
    "4. **Pattern detection**: Find cyclical behavior\n",
    "\n",
    "**Expected output**: DataFrame with 5 columns showing current week and 4 lagged versions, with NaN in early rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = (df.set_index(\"dateSeen\").resample(\"W\")[\"discount_percent\"].mean()).dropna()\n",
    "lag_df = pd.DataFrame({\"current\": ts})\n",
    "\n",
    "lag_df[\"lag_1\"] = ts.shift(1)\n",
    "lag_df[\"lag_3\"] = ts.shift(3)\n",
    "lag_df[\"lag_6\"] = ts.shift(6)\n",
    "lag_df[\"lag_12\"] = ts.shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßπ Cell 18: Remove Missing Values from Lag Data\n",
    "\n",
    "**Purpose**: Clean the lag DataFrame by removing rows with NaN values.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "```python\n",
    "lag_df = lag_df.dropna()\n",
    "```\n",
    "\n",
    "**Why this is necessary**:\n",
    "- The shift() operation creates NaN values:\n",
    "  - Lag 1: First 1 row has NaN\n",
    "  - Lag 3: First 3 rows have NaN\n",
    "  - Lag 12: First 12 rows have NaN\n",
    "- `dropna()`: Removes rows where ANY column has NaN\n",
    "- Result: Only complete cases remain\n",
    "\n",
    "**Impact**:\n",
    "- Loses first 12 weeks of data (due to lag_12)\n",
    "- Trade-off: Smaller dataset but complete feature set\n",
    "- For ~104 weeks (2 years), ~92 usable rows remain\n",
    "\n",
    "**Why we need complete cases**:\n",
    "- Required for correlation calculations\n",
    "- Necessary for scatter plots\n",
    "- Most models require complete data\n",
    "- Ensures fair comparison across all lags\n",
    "\n",
    "**Expected output**: Cleaned lag_df with no NaN values, ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = lag_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìâ Cell 19: Lag-1 Autocorrelation Plot\n",
    "\n",
    "**Purpose**: Visualize the relationship between current discount values and values from 1 week ago.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(lag_df[\"lag_1\"], lag_df[\"current\"])\n",
    "plt.xlabel(\"Lag 1 (t-1)\")\n",
    "plt.ylabel(\"Current (t)\")\n",
    "plt.title(\"Lag-1 Relationship\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Components**:\n",
    "- **Square plot** (6√ó6): Equal scales for x and y\n",
    "- **X-axis**: Last week's discount (t-1)\n",
    "- **Y-axis**: This week's discount (t)\n",
    "- **Each point**: One week of data\n",
    "\n",
    "**How to interpret patterns**:\n",
    "\n",
    "**1. Strong Positive Correlation** (upward diagonal):\n",
    "```\n",
    "  High current\n",
    "       ‚Üë\n",
    "       |    /\n",
    "       |   /\n",
    "       |  /\n",
    "       | /\n",
    "       |/________ High lag_1 ‚Üí\n",
    "```\n",
    "- Points cluster along diagonal\n",
    "- High lag_1 ‚Üí High current\n",
    "- **Meaning**: Discounts persist week-to-week\n",
    "- **SARIMA**: Need AR(1) component\n",
    "\n",
    "**2. No Correlation** (random cloud):\n",
    "```\n",
    "       ‚Üë  ‚Ä¢ ‚Ä¢\n",
    "       | ‚Ä¢  ‚Ä¢ ‚Ä¢\n",
    "       |  ‚Ä¢  ‚Ä¢\n",
    "       |‚Ä¢ ‚Ä¢   ‚Ä¢\n",
    "       |________ ‚Üí\n",
    "```\n",
    "- Points scattered randomly\n",
    "- **Meaning**: Last week doesn't predict this week\n",
    "- **SARIMA**: Try different lags\n",
    "\n",
    "**3. Negative Correlation** (downward diagonal):\n",
    "```\n",
    "       ‚Üë\\\n",
    "       | \\\n",
    "       |  \\\n",
    "       |   \\\n",
    "       |    \\\n",
    "       |________ ‚Üí\n",
    "```\n",
    "- High lag_1 ‚Üí Low current\n",
    "- **Meaning**: Alternating pattern\n",
    "- **Rare** in discount data\n",
    "\n",
    "**What this tells us**:\n",
    "\n",
    "- **Strong correlation**: \n",
    "  - Discounts are predictable\n",
    "  - Include AR terms in SARIMA\n",
    "  - Good 1-week-ahead forecasts possible\n",
    "\n",
    "- **Weak correlation**:\n",
    "  - Check other lags (3, 6, 12)\n",
    "  - May need seasonal components\n",
    "  - More challenging to forecast\n",
    "\n",
    "**Connection to PACF**:\n",
    "- This plot visualizes what PACF measures\n",
    "- Helps validate PACF analysis\n",
    "- Guides AR order selection\n",
    "\n",
    "**Optional enhancement** (could add):\n",
    "```python\n",
    "correlation = lag_df[\"lag_1\"].corr(lag_df[\"current\"])\n",
    "print(f\"Lag-1 correlation: {correlation:.3f}\")\n",
    "```\n",
    "\n",
    "**Expected output**: Scatter plot revealing the strength and direction of week-to-week persistence in discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(lag_df[\"lag_1\"], lag_df[\"current\"])\n",
    "plt.xlabel(\"Lag 1 (t-1)\")\n",
    "plt.ylabel(\"Current (t)\")\n",
    "plt.title(\"Lag-1 Relationship\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary and Next Steps\n",
    "\n",
    "### What This Notebook Accomplished:\n",
    "\n",
    "#### ‚úÖ Data Preparation\n",
    "- Loaded and cleaned electronics pricing dataset\n",
    "- Removed 12 unnecessary columns\n",
    "- Handled missing values strategically\n",
    "- Converted dates to proper datetime format\n",
    "- Created discount percentage metric\n",
    "\n",
    "#### ‚úÖ Feature Engineering\n",
    "- Calculated price differences and discount percentages\n",
    "- Extracted temporal features (year, month)\n",
    "- Created lag features (1, 3, 6, 12 weeks)\n",
    "- Filtered to high-quality time period (2017-2018)\n",
    "\n",
    "#### ‚úÖ Exploratory Analysis\n",
    "- Correlation heatmap of numerical features\n",
    "- 4-panel pricing and discount dashboard\n",
    "- Seasonal pattern detection (monthly trends)\n",
    "- Year-over-year comparison\n",
    "\n",
    "#### ‚úÖ Time Series Preparation\n",
    "- Weekly resampling for noise reduction\n",
    "- Trend visualization across three date perspectives\n",
    "- Pivot table creation (month √ó year)\n",
    "- Seasonality heatmap\n",
    "\n",
    "#### ‚úÖ Autocorrelation Analysis\n",
    "- Lag feature creation\n",
    "- Lag-1 scatter plot\n",
    "- Temporal dependency visualization\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Findings:\n",
    "\n",
    "1. **Seasonality Confirmed**: Clear monthly patterns with peaks in Nov-Dec\n",
    "2. **Data Quality**: 2017-2018 provide most reliable data\n",
    "3. **Multiple Perspectives**: Three date fields offer robust cross-validation\n",
    "4. **Product Variety**: Some products have extensive price histories\n",
    "5. **Discount Patterns**: Regular seasonal cycles evident\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Ready for SARIMA Modeling:\n",
    "\n",
    "This notebook has prepared the data for SARIMA forecasting:\n",
    "\n",
    "1. ‚úÖ **Clean time series**: Weekly discount percentages\n",
    "2. ‚úÖ **Seasonality identified**: Monthly patterns (s=12 for monthly, s=52 for weekly)\n",
    "3. ‚úÖ **Temporal features**: Ready for decomposition\n",
    "4. ‚úÖ **Lag analysis**: Autocorrelation insights\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Next Steps for SARIMA:\n",
    "\n",
    "1. **Stationarity Testing**:\n",
    "   - Run Augmented Dickey-Fuller test\n",
    "   - Apply differencing if needed\n",
    "   - Confirm stationarity\n",
    "\n",
    "2. **ACF/PACF Analysis**:\n",
    "   - Plot autocorrelation functions\n",
    "   - Identify p, d, q parameters\n",
    "   - Determine P, D, Q, s for seasonal component\n",
    "\n",
    "3. **Model Building**:\n",
    "   - Fit SARIMA(p,d,q)(P,D,Q,s)\n",
    "   - Test multiple parameter combinations\n",
    "   - Use AIC/BIC for model selection\n",
    "\n",
    "4. **Model Validation**:\n",
    "   - Check residuals (white noise test)\n",
    "   - Diagnostic plots\n",
    "   - Cross-validation\n",
    "\n",
    "5. **Forecasting**:\n",
    "   - Generate predictions\n",
    "   - Create confidence intervals\n",
    "   - Visualize forecasts vs actuals\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Recommended SARIMA Starting Point:\n",
    "\n",
    "Based on the analysis:\n",
    "- **For weekly data**: SARIMA(1,1,1)(1,1,1,52) - 52-week seasonal cycle\n",
    "- **For monthly data**: SARIMA(1,1,1)(1,1,1,12) - 12-month seasonal cycle\n",
    "\n",
    "Adjust based on ADF test and ACF/PACF analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Libraries Used:\n",
    "- **pandas**: Data manipulation\n",
    "- **matplotlib**: Visualization\n",
    "- **seaborn**: Statistical plots\n",
    "- **statsmodels** (ready for next phase): SARIMA modeling\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook provides a comprehensive foundation for time series forecasting of electronics product discounts!** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Absolute Percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
