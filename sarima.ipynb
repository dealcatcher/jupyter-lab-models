{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Time Series Analysis: Electronics Product Pricing Data\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive time series analysis on electronics product pricing data with the goal of understanding discount patterns, detecting seasonality, and preparing data for SARIMA forecasting.\n",
    "\n",
    "### Analysis Components:\n",
    "1. **Data Loading and Preprocessing**\n",
    "2. **Feature Engineering** (discount calculations, temporal features)\n",
    "3. **Exploratory Data Analysis** (correlations, visualizations)\n",
    "4. **Seasonality Detection** (monthly and weekly patterns)\n",
    "5. **Rolling Statistics** (weekly aggregations)\n",
    "6. **Lag Feature Engineering** (autocorrelation analysis)\n",
    "\n",
    "### Dataset:\n",
    "- **Source**: ElectronicsProductsPricingData.csv\n",
    "- **Time Period**: 2014-2018 (focus on 2017-2018)\n",
    "- **Key Metric**: Discount percentage over time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Loading\n",
    "\n",
    "This section imports the required libraries and loads the dataset into a pandas DataFrame. The dataset is the foundation for all subsequent preprocessing and analysis steps. We will install all the libraries that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Installation\n",
    "\n",
    "**Purpose**: Install required Python packages for time series analysis.\n",
    "\n",
    "**Libraries needed**:\n",
    "- **`statsmodels`**: Statistical models for time series (ARIMA, SARIMA, ADF test, ACF/PACF)\n",
    "- **`pandas`**: Data manipulation and analysis with DataFrames\n",
    "- **`matplotlib`**: Core plotting library for visualizations\n",
    "- **`seaborn`**: Statistical visualization built on matplotlib\n",
    "\n",
    "**Installation command**:\n",
    "```bash\n",
    "pip install statsmodels pandas matplotlib seaborn\n",
    "```\n",
    "\n",
    "**Note**: This cell is commented out because package installation should typically be done via terminal/command prompt, not within the notebook itself. Uncomment only if running in an environment where pip install in cells is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install statsmodels pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Filtering data \n",
    "\n",
    "### Load data\n",
    "Reads a CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Display Info\n",
    "\n",
    "**Purpose**: Import necessary libraries, load the electronics pricing dataset, and inspect its structure.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Import Core Libraries**:\n",
    "\n",
    "2. **Load CSV File**:\n",
    "\n",
    "3. **Display Dataset Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./ElectronicsProductsPricingData.csv\", encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Columns\n",
    "\n",
    "**Purpose**: Clean the dataset by dropping columns that are not needed for price and discount analysis.\n",
    "\n",
    "**Columns being removed**:\n",
    "\n",
    "1. **`Unnamed: 26-30`**: Empty columns likely created during data export\n",
    "2. **`sourceURLs`**: Web source links (not needed for analysis)\n",
    "3. **`prices.currency`**: Currency type (assuming all same currency)\n",
    "4. **`keys`**: Internal database keys\n",
    "5. **`ean`**: European Article Number (barcode)- it contains 5706\n",
    "6. **`prices.shipping`**: Shipping costs (focusing on product price only), and it contaims 2972 null values\n",
    "7. **`manufacturerNumber`**: Manufacturer-specific product codes\n",
    "8. **`upc`**: Universal Product Code (another barcode standard)\n",
    "9. **`manufactureer`**- it also contains 4014 missing values \n",
    "**Why remove these columns**:\n",
    "- **Reduces memory**: Smaller dataset is faster to process\n",
    "- **Improves clarity**: Easier to see relevant columns\n",
    "- **Focuses analysis**: Only keeps features needed for time series modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df.isnull().sum()\n",
    "\n",
    "## removing all the unnecessary columns from the dataframe\n",
    "df.drop(columns=[\"Unnamed: 26\",\"Unnamed: 27\",\"Unnamed: 28\",\"Unnamed: 29\",\"Unnamed: 30\",\"sourceURLs\",\"prices.currency\", \"keys\",\"ean\",\"prices.shipping\",\"manufacturerNumber\",\"upc\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Price Difference and Discount Percentage\n",
    "\n",
    "**Purpose**: Create new features to quantify discounts for each product.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Remove rows with missing prices**:\n",
    "\n",
    "2. **Calculate absolute price difference**:\n",
    "   \n",
    "3. **Calculate discount percentage**:\n",
    "   \n",
    "4. **Display updated info**:\n",
    "   \n",
    "\n",
    "**Why discount percentage matters**:\n",
    "- **Time series target**: This is the primary metric we'll forecast\n",
    "- **Comparability**: 20% discount means the same whether on $10 or $1000 item\n",
    "- **Business relevance**: Reflects actual promotional strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##1.2 we will check if the prices amountMax and minAmount is same \n",
    "## we have also put a new column discount percentage.\n",
    "\n",
    "df.dropna(subset=[\"prices.amountMax\", \"prices.amountMin\"], inplace=True)\n",
    "df[\"price_difference\"] = df[\"prices.amountMax\"] - df[\"prices.amountMin\"]\n",
    "df['discount_percent'] = (df['price_difference'] / df['prices.amountMax']) * 100\n",
    "\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Product ID Distribution\n",
    "\n",
    "**Purpose**: Identify which products have multiple price entries (good candidates for time series analysis).\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "- `value_counts()`: Counts occurrences of each unique product ID\n",
    "- Returns a Series sorted by frequency (descending)\n",
    "- Shows which products appear most often in the dataset\n",
    "\n",
    "**What the output tells us**:\n",
    "\n",
    "- **High counts** (e.g., 150+ entries):\n",
    "  - Product with extensive price history\n",
    "  - Multiple observations over time\n",
    "  - **Excellent for time series modeling**\n",
    "  - Can detect trends and seasonality\n",
    "\n",
    "- **Medium counts** (e.g., 20-50 entries):\n",
    "  - Reasonable historical data\n",
    "  - Sufficient for basic trend analysis\n",
    "\n",
    "- **Low counts** (e.g., 1-5 entries):\n",
    "  - Limited history\n",
    "  - Not suitable for time series modeling\n",
    "  - May be new products or single observations\n",
    "\n",
    "**Why this matters**:\n",
    "- **Product selection**: Helps choose which products to analyze in detail\n",
    "- **Data richness**: Products with more entries provide better forecasts\n",
    "- **Time series viability**: Need sufficient data points for SARIMA models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will check if the data has unique ids \n",
    "df[\"id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Specific Product Case Study\n",
    "\n",
    "**Purpose**: Focus on one product with rich historical data and filter for sale periods.\n",
    "\n",
    "\n",
    "\n",
    "1. **Filter by specific product ID**:\n",
    "2. **Filter for sale items only**:\n",
    "3. **Display results**:\n",
    "  \n",
    "\n",
    "**Why this approach**:\n",
    "\n",
    "- **Case study methodology**: Deep dive into one product before generalizing\n",
    "- **Data richness**: This product has sufficient history for analysis\n",
    "- **Sale focus**: Understand promotional pricing strategy specifically\n",
    "- **Simpler modeling**: Single product is easier than multi-product aggregation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking in the dataframe electronics if there are different products\n",
    "## from the above category, we have checked if the dataframe has different ids, and then checked how many values each id has, where we found one particular interested one. \n",
    "\n",
    "\n",
    "df_id = df[df[\"id\"] == \"AV1YFZVDvKc47QAVgp7V\"]\n",
    "df_id = df_id[df_id[\"prices.isSale\"] == True]\n",
    "df_id.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Format and Discount Calculation\n",
    "\n",
    "Proper date handling is crucial for time series analysis. This section converts string dates to datetime objects and extracts temporal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Parsing and Temporal Feature Engineering\n",
    "\n",
    "**Purpose**: Convert date strings to datetime objects and extract year components for temporal analysis.\n",
    "\n",
    "1. **Convert three date columns to datetime format**:\n",
    "   \n",
    "   \n",
    "   **Each date field serves a different purpose**:\n",
    "   - **`dateSeen`**: When the price was actually observed/scraped\n",
    "     - Most relevant for understanding consumer-facing prices\n",
    "     - Best for analyzing real-world pricing trends\n",
    "   \n",
    "   - **`dateUpdated`**: When the database record was last modified\n",
    "     - Shows data freshness\n",
    "     - May lag behind actual price changes\n",
    "   \n",
    "   - **`dateAdded`**: When product first entered the database\n",
    "     - Useful for understanding product lifecycle\n",
    "     - Initial pricing analysis\n",
    "   \n",
    "   **`errors=\"coerce\"`**: Invalid dates become NaT (Not a Time) instead of raising errors\n",
    "\n",
    "2. **Extract year from each datetime column**:\n",
    "  \n",
    "   - `.dt.year`: Accessor for datetime year component\n",
    "   - Creates integer columns (2014, 2015, 2016, etc.)\n",
    "   - Enables easy year-based filtering and grouping\n",
    "\n",
    "3. **Check year distribution**:\n",
    " \n",
    "   - Shows how many records per year\n",
    "   - Identifies temporal coverage of dataset\n",
    "   - Helps decide which years to focus on\n",
    "\n",
    "4. **Data validation - remove unparseable dates**:\n",
    "   \n",
    "5. **Remove redundant original column**:\n",
    "  \n",
    "6. **Display cleaned data**:\n",
    "  \n",
    "\n",
    "**Why datetime conversion matters**:\n",
    "- **Enables time-based operations**:\n",
    "  - Sorting chronologically\n",
    "  - Resampling (daily â†’ weekly â†’ monthly)\n",
    "  - Date filtering and slicing\n",
    "  - Time difference calculations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## converting the date columns (which are in strings) to datetime format\n",
    "df[\"dateSeen\"] = pd.to_datetime(df[\"prices.dateSeen\"], errors=\"coerce\")\n",
    "df[\"dateUpdated\"] = pd.to_datetime(df[\"dateUpdated\"], errors=\"coerce\")\n",
    "df[\"dateAdded\"] = pd.to_datetime(df[\"dateAdded\"], errors=\"coerce\")\n",
    "\n",
    "## extracting the year from the date column\n",
    "df[\"date_year\"] = df[\"dateSeen\"].dt.year\n",
    "df[\"updated_year\"] = df[\"dateUpdated\"].dt.year\n",
    "df[\"added_year\"] = df[\"dateAdded\"].dt.year\n",
    "\n",
    "df[\"date_year\"].value_counts()\n",
    "# 2. Drop rows where dates couldn't be parsed (Cleaning the dataset)\n",
    "# This is part of your \"Data validation pipeline\" milestone \n",
    "df = df.dropna(subset=['dateAdded', 'dateUpdated'])\n",
    "\n",
    "## we delete the column prices.dateSeen \n",
    "df.drop(columns=['prices.dateSeen'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seting time index with sorting it with date and discount percent \n",
    "since it  is the loading phase , here we will set an index with dateAdded , and sort the index with date also. \n",
    "\n",
    "**Purpose** \n",
    " - Lag features depends on the correct order , if the date is not sorted , it will give the price_lag_1 wrong , the rolling averages and discount detection will also be incoreect and inaccurate .\n",
    " - the rows are sorted first with dateAdded and then it will sorted with discount percent \n",
    "\n",
    "**df_index_dateSeen** - this will be new dataset just for the index \n",
    "\n",
    "**What have we done in the data** \n",
    "1. we will first create a new dataset, where we will set dateSeen as the new index, we will aslo sort that dataset with dateSeen index and then with discount_percent\n",
    "2. we will also sort the original datset with dateseen and then with discount_percent\n",
    "\n",
    " the reason we have create two different dataset , it is because we cam not use the dateSeen column now, because it is a index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_dateSeen = df.set_index(\"dateSeen\").sort_index()\n",
    "df_index_dateSeen.sort_values(by=[\"dateSeen\",\"discount_percent\"], inplace=True)\n",
    "df.sort_values(by=[\"dateSeen\",\"discount_percent\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Missing Values and outliners \n",
    "here we will check if we have any missing values , missing dates and missing ids ,furthermore , the problem we get is what type og missing values do we have \n",
    "\n",
    "**Outliners** \n",
    " - sometimes outliers in Price or Discount data can be just as damaging as missing values because they skew your averages and break your machine learning models.\n",
    " - we will use the the \"rule of thumb\" detection(z-score) to find outliners, here we will look for values that are more then 3 S.D away from the mean.\n",
    " - we will also visulize the outliners in the dataset, for this boxplots are good , if we see dots very far away from the box , those are our promary targets for removal or correction. \n",
    "\n",
    "**Results**\n",
    " - we do not have rows that have null dates and null prices values, and if we did have the missing values , we would have used the forward_fill backward_fill and interpolation. \n",
    " - The quickest way to see if you have \"impossible\" prices or discounts is the .describe() method.\n",
    " - since the maxprice was between 0 to 5000 and the minprice was between the 0 to 2500, furthermore for the amountmax and amountmin is both 0 \n",
    " - since we also have check if there is any null values in the price.amountMax and price.amountMin and dates , furthermore , we have checked if there was any zero value in the datset , but we did not find any, the minimum value for both priceMax or priceMin\n",
    " - furthermore , we have that the maximum percentage was around 80 percent.\n",
    " - \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "# Simple Z-score logic\n",
    "mean = df['discount_percent'].mean()\n",
    "std =  df['discount_percent'].std()\n",
    "\n",
    "# Anything outside this range is a statistical outlier\n",
    "upper_limit = mean + (3 * std)\n",
    "lower_limit = mean - (3 * std)\n",
    "\n",
    "outliers = df[(df['discount_percent'] > upper_limit) | (df['discount_percent'] < lower_limit)]\n",
    "\n",
    "print(f\"Detected {len(outliers)} outliers.\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x=df['discount_percent'], color='skyblue',flierprops={'markerfacecolor':'red', 'markersize': 10})\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=outliers['discount_percent'], color='lightcoral',flierprops={'markerfacecolor':'blue', 'markersize': 10})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"dateSeen\",\"discount_percent\"], inplace=True,ascending=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DATA Visualization \n",
    "\n",
    "Comprehensive visualizations to understand pricing patterns, discount trends, and temporal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Multi-Dimensional Discount Analysis\n",
    "To identify price volatility and seasonal discounting cycles, we implemented a 2x2 visualization matrix. This aligns with the Week 3-4 Milestone of performing deep exploratory data analysis (EDA) to justify the transition to predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2x2 frame\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('DealCatcher EDA: Price & Discount Trends', fontsize=20)\n",
    "\n",
    "# 1. Scatterplot of Min-Max Prices\n",
    "sns.scatterplot(ax=axes[0, 0], data=df, x='prices.amountMin', y='prices.amountMax', alpha=0.5)\n",
    "axes[0, 0].set_title('Scatterplot of Min-Max Prices')\n",
    "\n",
    "# 2. Discount% vs Date Added\n",
    "sns.scatterplot(ax=axes[0, 1], data=df, x='dateAdded', y='discount_percent', color='orange')\n",
    "axes[0, 1].set_title('Discount % vs Date Added')\n",
    "\n",
    "# 3. Discount% vs Date Last Seen\n",
    "sns.scatterplot(ax=axes[1, 0], data=df, x='dateSeen', y='discount_percent', color='green')\n",
    "axes[1, 0].set_title('Discount % vs Date Seen')\n",
    "\n",
    "# 4. Discount% vs Date Updated\n",
    "sns.scatterplot(ax=axes[1, 1], data=df, x='dateUpdated', y='discount_percent', color='red')\n",
    "axes[1, 1].set_title('Discount % vs Date Updated')\n",
    "\n",
    "# Rotate labels so they don't overlap\n",
    "for ax in axes.flat:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Overall Price Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Seasonal Discount Analysis (2017-2018)\n",
    "\n",
    "**Purpose**: Filter data to 2017-2018 and create two plots showing seasonal discount patterns from different date perspectives.\n",
    "\n",
    "**Part 1: Data Filtering**\n",
    "\n",
    "**Why filter to 2017-2018?**\n",
    "- Earlier analysis showed 2014-2015 had sparse data\n",
    "- 2017-2018 have the highest data density\n",
    "- Ensures all three date fields are within range\n",
    "- Focuses on most recent and complete period\n",
    "\n",
    "**Part 2: Plot 1 - Seasonality by Date Seen**\n",
    "\n",
    "**What this does**:\n",
    "- Extracts month (1-12) from dateSeen\n",
    "- Creates line plot with:\n",
    "  - X-axis: Months (1=Jan, 12=Dec)\n",
    "  - Y-axis: Average discount percentage\n",
    "  - Two lines (one for 2017, one for 2018)\n",
    "  - `marker=\"o\"`: Circle at each month\n",
    "\n",
    "**How to interpret**:\n",
    "- **Peaks**: Months with highest discounts (likely Nov-Dec)\n",
    "- **Valleys**: Months with lowest discounts (likely Jan-Feb)\n",
    "- **Line comparison**: Year-over-year consistency\n",
    "\n",
    "\n",
    "**What this shows**:\n",
    " - 1ï¸âƒ£ Strong early-year discounts in 2018\n",
    "    - Janâ€“April 2018 shows high discounts\n",
    "    - Peak around March (~11%)\n",
    "    - Then sharp drop in Mayâ€“June\n",
    " - 2ï¸âƒ£ 2017 shows strong late-year discounts\n",
    "    - Very low discounts in early 2017\n",
    "    - Gradual rise from July onward\n",
    "    - Peak around November (~10%)\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**: Two line plots showing monthly discount percentage trends with year-over-year comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove the years 2014 and 2015 from the dataset as they have very few data points and we will focus on the years 2017 and 2018 for our seasonality analysis.\n",
    "\n",
    "\n",
    "## here we will change the data from the electronics dataset to only include the years 2017 and 2018 for the date seen, date updated and date added columns as they have more data points and we will focus on these years for our seasonality analysis.\n",
    "df = df[df[\"updated_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"date_year\"].isin([2017, 2018])]\n",
    "df = df[df[\"added_year\"].isin([2017, 2018])]\n",
    "\n",
    "df[\"month\"] = df[\"dateSeen\"].dt.month\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "sns.lineplot(data=df, x=\"month\", y=\"discount_percent\", hue=\"date_year\", marker=\"o\")\n",
    "plt.title(\"Discount Seasonality using Date Seen (2017â€“2018)\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekday vs the Weeknd \n",
    "This is a feature engineering step. it is modeling step, helps model learn patterns , uses engineered columns , used for prediction , helps  model exploit seasonality.\n",
    " - **Results**\n",
    "- This column contains both 2017 and 2018 data ,, weekdays has more dicsount then the Weeknd ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and day of week information\n",
    "df[\"year\"] = df[\"dateSeen\"].dt.year\n",
    "df[\"day_of_week\"] = df[\"dateSeen\"].dt.day_name()\n",
    "df[\"is_weeknd\"] = df[\"day_of_week\"].isin([\"Saturday\", \"Sunday\"])\n",
    "df[\"is_weeknd\"] = df[\"is_weeknd\"].replace({True: \"Weekend\", False: \"Weekday\"})\n",
    "\n",
    "# Group by both day and year\n",
    "df.groupby([\"year\", \"day_of_week\"])[\"discount_percent\"].mean()\n",
    "\n",
    "# Plot 1: Discount by Day of Week - comparing 2017 vs 2018\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=df, \n",
    "    x=\"day_of_week\", \n",
    "    y=\"discount_percent\", \n",
    "    hue=\"year\",  # This creates separate bars for each year\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.7, \n",
    "    edgecolor=\"black\",\n",
    "    order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    ")\n",
    "plt.title(\"Average Discount Percent by Day of Week (2017 vs 2018)\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Average Discount Percent\")\n",
    "plt.legend(title=\"Year\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Weekend vs Weekday - comparing 2017 vs 2018\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=df,\n",
    "    x=\"is_weeknd\", \n",
    "    y=\"discount_percent\",\n",
    "    hue=\"year\",  # This creates separate bars for each year\n",
    "    palette=\"Set2\", \n",
    "    alpha=0.7, \n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "plt.title(\"Average Discount Percent: Weekend vs Weekday (2017 vs 2018)\")\n",
    "plt.xlabel(\"Day Type\")\n",
    "plt.ylabel(\"Average Discount Percent\")\n",
    "plt.legend(title=\"Year\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 Discount Seasonality \n",
    " - This is EDA(Exploratory data analysies), furthermore , it is a analytical step , helps you understand data, uses acf , plots no prediction involved , it will answer wether there is seasonlity\n",
    " - always 1 (a series is perfectly coorlelated with itself)\n",
    " - lag 1 (todays price is stronly corelated to yesterday price)\n",
    " - slow decay pattern (the autoccrealtion gradually declines instaed of cutting off quickly), which suggests that series has trend, the series is non-stationary, and prices discount changes slowly over time\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"discount_percent\"] > 0]\n",
    "plot_acf(df[\"discount_percent\"], lags=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pacf \n",
    "- **Lag 2** - smaller that lag 1 , but clearly outside confidence band.\n",
    "- After lag 2-3 , values drops near zero, most lags after 3 are very small and within confidence bounds , which is critical.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_pacf(df[\"discount_percent\"], lags=60, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdFuller - for the non-stattionaery \n",
    "adfuller - used to perform the Augmented Dickey-Fuller test, which is a statistical test for testing the stationarity of a time series\n",
    "\n",
    "Stationarity implies that the statistical properties of the time series, such as mean and variance, do not change over time.\n",
    "\n",
    "The test evaluates a null hypothesis that the time series is non-stationary. If the p-value obtained from the test is less than a significance level (commonly 0.05), we reject the null hypothesis and conclude that the time series is stationary.\n",
    " - null hypothesies \n",
    " - unit root test ( if a time series is non-stationary or not), \n",
    " - here the adf test for unit roots , not general stationarity , \n",
    "\n",
    "if the p-value is less then 0.05 , then we reject thr n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"discount_percent\"], inplace=True)\n",
    "result = adfuller(df[\"discount_percent\"].dropna())\n",
    "\n",
    "\n",
    "\n",
    "print(\"ADF Statistic:\", result[0])\n",
    "print(\"p-value:\", result[1])\n",
    "print(\"Lags Used:\", result[2])\n",
    "print(\"Number of Observations:\", result[3])\n",
    "print(\"Critical Values:\")\n",
    "\n",
    "for key, value in result[4].items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Month Ã— Year Pivot Table\n",
    "\n",
    "**Purpose**: Reorganize data into a pivot table with months as rows and years as columns.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Set dateSeen as index and sort chronologically**:\n",
    "2. **Create pivot table**:\n",
    "\n",
    "\n",
    "**Parameters explained**:\n",
    "- `values=\"discount_percent\"`: The metric to aggregate\n",
    "- `index=...index.month`: Rows = months (1-12)\n",
    "- `columns=...index.year`: Columns = years (2017, 2018)\n",
    "- `aggfunc=\"mean\"`: Calculate average discount per cell\n",
    "\n",
    "\n",
    "\n",
    "**Why this format is useful**:\n",
    "- **Easy comparison**: See 2017 vs 2018 side-by-side\n",
    "- **Heatmap ready**: Perfect structure for next visualization\n",
    "- **Pattern detection**: Scan vertically for seasonal patterns\n",
    "- **Year-over-year**: Compare horizontally\n",
    "\n",
    "**Troubleshooting note**:\n",
    "Comment mentions possible error if `prices.dateSeen` doesn't exist - this is because we already converted it to `dateSeen` in earlier cells.\n",
    "\n",
    "**Expected output**: A 12Ã—2 table showing average discount percentage for each month in 2017 and 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## if you got this error \"None of ['prices.dateSeen'] are in the columns\"\n",
    "## then uncomment the above line \n",
    "plt.figure(figsize=(12, 6))\n",
    "df_pivot_table = df.pivot_table(values=\"discount_percent\", index=df_index_dateSeen.index.month, columns=df_index_dateSeen.index.year, aggfunc=\"mean\")\n",
    "print(df_pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Monthly Discount Patterns\n",
    "\n",
    "**Purpose**: Visualize the pivot table as a color-coded heatmap for easy pattern recognition.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "**Parameters**:\n",
    "- `df_pivot_table`: The month Ã— year data from previous cell\n",
    "- `annot=True`: Display discount percentages in each cell\n",
    "- `cmap=\"coolwarm\"`: Color map\n",
    "  - **Blue** = Lower discounts (cool colors)\n",
    "  - **Red** = Higher discounts (warm colors)\n",
    "  - **White** = Mid-range\n",
    "- `linewidths=0.5`: Thin white lines separating cells\n",
    "\n",
    "\n",
    "**Expected patterns**:\n",
    "- **Red cluster**: November-December (Black Friday, Christmas)\n",
    "- **Blue cluster**: January-February (post-holiday)\n",
    "- **Consistent columns**: Similar color progression in both years\n",
    "\n",
    "**Business insights**:\n",
    "- **Promotional calendar**: When to plan major sales\n",
    "- **Inventory planning**: Anticipate discount periods\n",
    "- **Competitive timing**: Align with industry patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_pivot_table, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuring enginerring pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Statistics\n",
    "\n",
    "Rolling statistics (moving averages) smooth short-term fluctuations and highlight longer-term trends. Essential for:\n",
    "- Noise reduction\n",
    "- Trend identification\n",
    "- Seasonality detection\n",
    "- Preparing data for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Features \n",
    "\n",
    "Lag features represent previous values in a time series. They are crucial for:\n",
    "- **Autocorrelation analysis**: Understanding temporal dependencies\n",
    "- **ARIMA/SARIMA modeling**: Determining AR order\n",
    "- **Machine learning**: Using past to predict future\n",
    "- **Pattern detection**: Identifying cyclical behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag Features\n",
    "\n",
    "**Purpose**: Create lagged versions of the discount time series to analyze autocorrelation.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "1. **Create clean weekly time series**:\n",
    "- Sets dateSeen as index\n",
    "- Resamples to weekly frequency\n",
    "- Calculates average discount per week\n",
    "- Removes any NaN values\n",
    "- Creates clean time series `ts`\n",
    "\n",
    "2. **Initialize lag DataFrame with current values**:\n",
    "- Creates DataFrame with one column: \"current\"\n",
    "- Contains the weekly discount percentages\n",
    "- This represents time **t** (present)\n",
    "\n",
    "3. **Create Lag 1 (1 week ago)**:\n",
    "- `.shift(1)`: Moves all values down by 1 position\n",
    "- Shows last week's discount\n",
    "- First value becomes NaN (no previous week)\n",
    "\n",
    "**Why these specific lags?**\n",
    "- **Lag 1**: Immediate persistence (AR(1) component)\n",
    "- **Lag 3**: ~3-week cycles (common in retail)\n",
    "- **Lag 6**: ~1.5 month patterns\n",
    "- **Lag 12**: Quarterly cycles (~3 months)\n",
    "\n",
    "**Purpose of lag features**:\n",
    "1. **Autocorrelation**: Do past values predict future?\n",
    "2. **ARIMA order**: Which lags are significant?\n",
    "3. **Forecasting**: Use historical data as predictors\n",
    "4. **Pattern detection**: Find cyclical behavior\n",
    "\n",
    "**Expected output**: DataFrame with 5 columns showing current week and 4 lagged versions, with NaN in early rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_dateSeen = df_index_dateSeen[df_index_dateSeen[\"discount_percent\"]>0]\n",
    "ts = (df_index_dateSeen.resample(\"D\")[\"discount_percent\"].mean()).dropna()\n",
    "lag_df = pd.DataFrame({\"current\": ts})\n",
    "\n",
    "lag_df[\"lag_1\"] = ts.shift(1)\n",
    "lag_df[\"lag_12\"] = ts.shift(12)\n",
    "lag_df[\"lag_14\"] = ts.shift(14)\n",
    "lag_df[\"lag_30\"] = ts.shift(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Missing Values from Lag Data\n",
    "\n",
    "**Purpose**: Clean the lag DataFrame by removing rows with NaN values.\n",
    "\n",
    "\n",
    "**Why this is necessary**:\n",
    "- The shift() operation creates NaN values:\n",
    "  - Lag 1: First 1 row has NaN\n",
    "  - Lag 3: First 3 rows have NaN\n",
    "  - Lag 12: First 12 rows have NaN\n",
    "- `dropna()`: Removes rows where ANY column has NaN\n",
    "- Result: Only complete cases remain\n",
    "\n",
    "**Impact**:\n",
    "- Loses first 12 weeks of data (due to lag_12)\n",
    "- Trade-off: Smaller dataset but complete feature set\n",
    "- For ~104 weeks (2 years), ~92 usable rows remain\n",
    "\n",
    "**Why we need complete cases**:\n",
    "- Required for correlation calculations\n",
    "- Necessary for scatter plots\n",
    "- Most models require complete data\n",
    "- Ensures fair comparison across all lags\n",
    "\n",
    "**Expected output**: Cleaned lag_df with no NaN values, ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = lag_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag-1 Autocorrelation Plot\n",
    "\n",
    "**Purpose**: Visualize the relationship between current discount values and values from 1 week ago.\n",
    "\n",
    "**What this code does**:\n",
    "\n",
    "**Components**:\n",
    "- **Square plot** (6Ã—6): Equal scales for x and y\n",
    "- **X-axis**: Last week's discount (t-1)\n",
    "- **Y-axis**: This week's discount (t)\n",
    "- **Each point**: One week of data\n",
    "\n",
    "**How to interpret patterns**:\n",
    "\n",
    "**1. Strong Positive Correlation** (upward diagonal):\n",
    "```\n",
    "  High current\n",
    "       â†‘\n",
    "       |    /\n",
    "       |   /\n",
    "       |  /\n",
    "       | /\n",
    "       |/________ High lag_1 â†’\n",
    "```\n",
    "- Points cluster along diagonal\n",
    "- High lag_1 â†’ High current\n",
    "- **Meaning**: Discounts persist week-to-week\n",
    "- **SARIMA**: Need AR(1) component\n",
    "\n",
    "**2. No Correlation** (random cloud):\n",
    "```\n",
    "       â†‘  â€¢ â€¢\n",
    "       | â€¢  â€¢ â€¢\n",
    "       |  â€¢  â€¢\n",
    "       |â€¢ â€¢   â€¢\n",
    "       |________ â†’\n",
    "```\n",
    "- Points scattered randomly\n",
    "- **Meaning**: Last week doesn't predict this week\n",
    "- **SARIMA**: Try different lags\n",
    "\n",
    "**3. Negative Correlation** (downward diagonal):\n",
    "```\n",
    "       â†‘\\\n",
    "       | \\\n",
    "       |  \\\n",
    "       |   \\\n",
    "       |    \\\n",
    "       |________ â†’\n",
    "```\n",
    "- High lag_1 â†’ Low current\n",
    "- **Meaning**: Alternating pattern\n",
    "- **Rare** in discount data\n",
    "\n",
    "**What this tells us**:\n",
    "\n",
    "- **Strong correlation**: \n",
    "  - Discounts are predictable\n",
    "  - Include AR terms in SARIMA\n",
    "  - Good 1-week-ahead forecasts possible\n",
    "\n",
    "- **Weak correlation**:\n",
    "  - Check other lags (3, 6, 12)\n",
    "  - May need seasonal components\n",
    "  - More challenging to forecast\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**: Scatter plot revealing the strength and direction of week-to-week persistence in discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(lag_df[\"lag_1\"], lag_df[\"current\"])\n",
    "plt.xlabel(\"Lag 1 (t-1)\")\n",
    "plt.ylabel(\"Current (t)\")\n",
    "plt.title(\"Lag-1 Relationship\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Next Steps\n",
    "\n",
    "### What This Notebook Accomplished:\n",
    "\n",
    "#### âœ… Data Preparation\n",
    "- Loaded and cleaned electronics pricing dataset\n",
    "- Removed 12 unnecessary columns\n",
    "- Handled missing values strategically\n",
    "- Converted dates to proper datetime format\n",
    "- Created discount percentage metric\n",
    "\n",
    "#### âœ… Feature Engineering\n",
    "- Calculated price differences and discount percentages\n",
    "- Extracted temporal features (year, month)\n",
    "- Created lag features (1, 3, 6, 12 weeks)\n",
    "- Filtered to high-quality time period (2017-2018)\n",
    "\n",
    "#### âœ… Exploratory Analysis\n",
    "- Correlation heatmap of numerical features\n",
    "- 4-panel pricing and discount dashboard\n",
    "- Seasonal pattern detection (monthly trends)\n",
    "- Year-over-year comparison\n",
    "\n",
    "#### âœ… Time Series Preparation\n",
    "- Weekly resampling for noise reduction\n",
    "- Trend visualization across three date perspectives\n",
    "- Pivot table creation (month Ã— year)\n",
    "- Seasonality heatmap\n",
    "\n",
    "#### âœ… Autocorrelation Analysis\n",
    "- Lag feature creation\n",
    "- Lag-1 scatter plot\n",
    "- Temporal dependency visualization\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Next Steps for SARIMA:\n",
    "\n",
    "Step 1: - \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Libraries Used:\n",
    "- **pandas**: Data manipulation\n",
    "- **matplotlib**: Visualization\n",
    "- **seaborn**: Statistical plots\n",
    "- **statsmodels** (ready for next phase): SARIMA modeling\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Absolute Percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
